{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the line below to install stanza\n",
    "#!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mari/miniconda3/lib/python3.10/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "2023-05-10 10:04:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bbaecf082f4658922e03fda868f807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0089e42afa8b41608ca27d9bde7a4d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/tokenize/combined.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c160d31abe47359c751843ef9ee081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/pos/combined.pt:   0%|         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d79285a6bce40b4bd1d5fb8f92e35c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/backward_charlm/1billion.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d345b669a544729c1f3142dd968e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/pretrain/combined.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fef8e845ee5485a9f749374ede8cecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/forward_charlm/1billion.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 10:04:31 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2023-05-10 10:04:31 INFO: Use device: cpu\n",
      "2023-05-10 10:04:31 INFO: Loading: tokenize\n",
      "2023-05-10 10:04:31 INFO: Loading: pos\n",
      "2023-05-10 10:04:32 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from util import spacy_get_sents, stanza_get_sents, spacy_tokenize_text, stanza_tokenize_text, get_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_tokens_dict, common_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our further inverstigations we will choose 100 texts from \"Writers\" category that we previously stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_texts(source_type='folder', path='Writers', token_type='file', n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the sample of 5 files from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Shahid Shafaat (Urdu: شاہد شفاعت) is a Pakistani director, writer, producer and actor who works in television, film and theatre. Mostly known for his work as a director, Shafaat started his on-screen career in the early 2000s and directed more than 10 television plays. He received Lux Style Award nomination as a Best TV Director for Dil Mom Ka Diya. Career Shafaat started his career in theatre. He directed several plays, most notably Main Adakara Banu Gi. He directed the series Kaafir in 2012, which received critical praise, but was a commercial failure. In 2016, he directed Next Level Entertainment's production, Khuda Mera Bhi Hai. The series received critical acclaim. In 2018, he directed Khasara, which was a commercial success. His next play was another Next level Entertainment production, Dil Mom Ka Diya which became the highest-rated play in Pakistani television history. He then ventured in social causes television series, including Surkh Chandni and Bikhray Moti . Filmography Film Jhol Television References \",\n",
       " 'Shay Youngblood (born 1959) is an American novelist, playwright, and author of short stories. Youngblood has worked as a public information assistant for WPBA in Atlanta and as a Peace Corps volunteer in Dominica. Early life and education Shay Youngblood was born in Columbus, Georgia, in 1959. Many elements of Youngblood\\'s life are reflected in her fiction. Like many of her heroines, Youngblood herself was an orphan at an early age. Her mother died when she about two years old, and she was raised by a community family: grandfathers, uncles and many women with similarities to those described in her books and plays. Youngblood was one of the first people in her family to attend college. While earning her bachelor\\'s degree in mass communication at Clark-Atlanta University, she participated in a service project in Haiti. Her work in Haiti heightened her awareness of the injustice suffered by poor people in many places around the world. Immediately after graduating she joined the Peace Corps, and in 1981 she served as an agricultural information officer in Dominica, in the Eastern Caribbean. Later on, in 1993, Shay earned her MFA in Creative Writing form Brown University. Career Youngblood is recognized as a poet, playwright, fiction writer, and has also written, produced, and directed two short videos. She is best known for her three texts, The Big Mama Stories, Soul Kiss and her most recent novel, Black Girl in Paris. Youngblood states that her first published text, The Big Mama Stories, is the closest to autobiographical of all of her works. This compilation of short stories focuses on the coming of age of a poor, young African-American girl named Chile. Chile\\'s biological mother, Fannie Mae, has died and so Chile and her brother go to live with a woman called \"Big Mama,\" who raises the children with the help of the entire community. Her fiction, articles and essays have been published in Oprah magazine, Good Housekeeping, BlackBook and Essence magazines, among many other publications. Her plays - Amazing Grace, Shakin\\' the Mess Outta Misery and Talking Bones - have been widely produced. Other plays by Youngblood include Black Power Barbie and Communism Killed My Dog. She also completed a radio play, Explain Me the Blues, for WBGO Public Radio\\'s Jazz Play Series. Youngblood is a board member of both Yaddo artists\\' colony and the Authors Guild. She has taught creative writing at NYU and was the 2002-03 John and Renee Grisham Writer in Residence at the University of Mississippi. She also taught writing at the Syracuse Community Writer\\'s Project as well as playwriting at the Rhode Island Adult Institution for Women and Brown University. She currently lives in Texas where she teaches creative writing at Texas A&M University. Awards Shay Youngblood has been the recipient of numerous grants and awards including a Pushcart Prize for her short story, \"Born With Religion.\" She has also received the Lorraine Hansberry Playwriting Award, the Astaea Writes\\' Award for Fiction, a 2004 New York Foundation for the Arts Sustained Achievement Award, an Edward Albee honoree, and several NAACP Theater Awards. References Further reading Hans Ostrom. \"Shay Youngblood,\" in The Greenwood Encyclopedia of African American Literature. Ed. Hans Ostrom and J. David Macey. Westport, CT: Greenwood Publishers, 2005. Volume 5, 1803-1804. External links Official website, containing a complete list of her published works Shay Youngblood Shay Youngblood VG: Artist Biography: Youngblood, Shay Women of Color Women of Word—African American Female Playwrights – Shay Youngblood',\n",
       " 'For the Arkansas politician, see Mary Bentley. Mary Ann Moore-Bentley, also known as Mary Ling (6 January 1865 – 1 September 1953), was an Australian writer and parliamentary candidate. Born in Braidwood to English-born Methodists George Bentley and Mary Ann, née Moore, young Mary and her two younger brothers was primarily educated at home by her mother. She and her sister visited the Sydney International Exhibition in 1879, but when their money ran out they were forced to work as domestic servants. In 1880 the family settled at Marrickville and Mary became a nursemaid to the children of Colonel Charles Roberts. She married postal clerk Henry Hill Ling on 3 September 1889 at the Salvation Army barracks in Burwood; they separated in 1897 and divorced in 1906.Moore-Bentley\\'s first novel was rejected in 1890; she published A Woman of Mars; or, Australia\\'s Enfranchised Woman in 1901. A Georgist, she joined the Single Tax League in 1901 and was appointed to its council, although she only attended two meetings. In 1903, under the name \"Mary Ann Moore Bentley\", she was one of four women to contest the 1903 federal election, the first at which women were eligible to stand, although she was not formally supported by the league. Contesting the Senate in New South Wales, she described herself as \"the working woman\\'s candidate\" and support free trade, abolition of state parliaments and a state bank in addition to Georgism. She received 18,924 votes (6.1%), outpolling the other New South Wales Senate candidate, Nellie Martel, by 400 votes.By 1906, Moore-Bentley\\'s relations with her brothers, her nearest neighbours at Bangor where she lived, grew tense. A Psychological Interpretation of the Gospel (January 1917) received a US publication in Boston and Moore-Bentley sailed to America later that year; she was repatriated at government expense in 1918 and blamed her disappointing time in America on the \"Secret Service\" and the Australian government\\'s misrepresentation of her anti-conscription activities. She retired to Menai, writing poems and children\\'s stories. In 1943 she was committed to the Mental Hospital at Stockton in Newcastle, where she died in 1953. Her memoir, Journey to Durran Durra 1852–1885, which was written around 1935, was published in 1983. References ',\n",
       " \"Charles Rayne Kruger (29 January 1922 – 21 December 2002) was a South African author and property developer. Charles Rayne Kruger was born on 29 January 1922 in Queenstown, in the Eastern Cape, the son of an unmarried 17-year-old daughter of a British Army officer. As his father had disappeared, his mother married Victor Kruger, a Johannesburg estate agent. He was educated at Jeppe High School and the University of the Witwatersrand.Kruger's first wife was the actress Nan Munro, a widow, 16 years older than him, with three children. They later divorced, and he married the restaurateur, chef and television presenter Prue Leith. They had a son, the Conservative MP Danny Kruger, and adopted a Cambodian daughter, Li-Da. Publications Tanker (Novel), London: Longman's Green & Co, 1952 The Spectacle (Crime story), London: Longman's Green & Co, 1953 Young Villain With Wings (Crime story), London: Longman's Green & Co, 1953 My Name Is Celia (Novel), London: Longman's Green & Co, 1954 The Even Keel (Crime story), London: Longman's Green & Co, 1955 Ferguson (Crime Story), London: Longman's Green & Co, 1956 Goodbye Dolly Gray: The Story of the Boer War. (Non fiction), London: Cassell, 1959 The Devil's Discus (Non fiction), London: Cassell, 1964 All Under Heaven: A Complete History of China. (Non fiction), Hoboken, New Jersey: John Wiley & Sons, 2003 References \",\n",
       " 'Alexander Dicsone (also Dicson and Dickson, Italian: Alessandro Dicsono) (1558–1603) was a Scottish writer and political agent. He is known also as the leading British disciple of Giordano Bruno. He used the pseudonym Heius Scepsius. Life Dicsone was born in Perthshire, and studied at the University of St Andrews. He became a follower and personal friend of Bruno, who was in England during the years 1583 to 1585. It is considered probable that they met in this period, though not certain. Dicsone in any case was then in England, and became the outstanding disciple of Bruno in England and Scotland. He is mentioned in Bruno\\'s dialogues, along with another British disciple (\"Smith\") who remains unidentified. Bruno and Dickson were part of the intellectual circle of Sir Philip Sidney.Dicsone opposed Ramism, and was attacked in the Antidicsonus by \"G.P.\" Now considered to be by William Perkins, it has also been attributed to Gerard Peeters. Walter Ong considered this dispute one of the major controversies over Ramism. Frances Yates argued that it should be considered as \"over-lapping\" with the debate of Bruno with the Aristotelians at Oxford, also in 1584. Perkins represented the Puritan view of mnemonic techniques based on images, which considered them tainted with idolatry, heresy, Catholicism and obscenity. With Bruno and Dicsone, Perkins mentioned in his dedicatory epistle Metrodorus of Scepsis and Cosma Rosselli.The memory technique taught by Dicsone was questioned by Hugh Plat in 1594. It has been suggested that Dicsone was led to Bruno\\'s memory theory by the requirement for memorable textbooks.Dicsone was said to have worked for Philip Sidney. By 1588 Dicsone was working for Francis Hay, 9th Earl of Erroll. Hay was a Catholic and rebel, and Dicsone acted as a go-between for his master and the Scottish Kirk. He was mixture of spy and double agent, a position eventually untenable.Dicsone was in trouble with James VI of Scotland for carrying letters from Charles Neville, 6th Earl of Westmorland, an English Catholic in exile. He declared himself a Catholic by the same year, 1591. He went on further continental travels, in the Catholic interest, with Peter Lowe. In the later 1590s James VI recruited him, and Dicsone wrote in James\\'s causes. According to the English diplomat George Nicholson, James VI employed Dicsone to write a treatise answering Doleman\\'s A Conference about the Next Succession and advancing the king\\'s title to the English throne. Dicsone was going through letters from Queen Elizabeth to help the argument in February 1598. His treatise titled Of the Right of the Crowne efter Hir Majesty was not published. Andrew Hunter wrote to Sir Robert Cecil in November 1598 that Dicsone was expected at The Hague and was an enemy to England. He is last heard of trying to bring John Davidson to heel, in 1603.The secretary of Anne of Denmark, William Fowler sent news of Dicsone\\'s death to Earl of Shrewsbury on 11 October 1603.Thomas Murray\\'s Elegy on his death appeared in 1604. His hermetic interests are considered an influence on Scottish \"mason craft\" (the precursor of freemasonry). Works De umbra rationis et judicii (1583). The Philological Museum; Alexander Dickson, De Umbra Rationis et Iudicii (1584), Online text Defensio pro Alexander Dicsono Arelio (1584). This work was under the pseudonym \"Heius Scepsius\", implying Dicsone was prepared to identify with (Metrodorus of) Scepsis, and accepted the \"criticism\" of using the signs of the Zodiac in memory technique.Dicsone dedicated his books to Robert Dudley, 1st Earl of Leicester. Notes ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our selected 100 articles we will perform sentence tokenization using Spacy and Stanza.\n",
    "Each file will be separated into sentences separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sents = [spacy_get_sents(x) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run for the first time Stanza tokenization:\n",
    "\"\"\" \n",
    "stanza_sents = [stanza_get_sents(x) for x in texts]\n",
    "with open('SpacyStanza/stanza_sents.pickle', 'wb') as f:\n",
    "    pickle.dump(stanza_sents, f))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# To load Stanza tokens from a pickle file\n",
    "\n",
    "with open('SpacyStanza/stanza_sents.pickle', 'rb') as f:\n",
    "    stanza_sents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now show the number of sentences calculated by each library for our articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spacy</th>\n",
       "      <th>Stanza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Spacy  Stanza\n",
       "0     107      95\n",
       "1      37      33\n",
       "2       8       9\n",
       "3      35      27\n",
       "4      28      29\n",
       "..    ...     ...\n",
       "95     32      30\n",
       "96     21      11\n",
       "97     18      17\n",
       "98      7       7\n",
       "99     24      22\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_df = pd.DataFrame(columns=['Spacy', 'Stanza'])\n",
    "\n",
    "sents_df.Spacy = list(map(lambda x: len(x), spacy_sents))\n",
    "sents_df.Stanza = list(map(lambda x: len(x), stanza_sents))\n",
    "\n",
    "sents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the stats for all articles, it appears that Spacy on average finds more sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spacy</th>\n",
       "      <th>Stanza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.770000</td>\n",
       "      <td>28.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.211978</td>\n",
       "      <td>24.465129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>21.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>155.000000</td>\n",
       "      <td>143.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Spacy      Stanza\n",
       "count  100.000000  100.000000\n",
       "mean    30.770000   28.230000\n",
       "std     26.211978   24.465129\n",
       "min      3.000000    3.000000\n",
       "25%     13.000000   11.000000\n",
       "50%     25.000000   21.500000\n",
       "75%     37.000000   33.250000\n",
       "max    155.000000  143.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the total number of sentences recognized by the both libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spacy     3077\n",
       "Stanza    2823\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find all unique sentences that both recognize.\n",
    "First we can look at the number shared sentences per article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_sents</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{In fact I won't sell to them., References, Ar...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{The Impaler – المُخوزِق., A historical novel ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{References, Also a documentary was made about...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{A statue of him, unveiled in 1950, stands at ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{References, His earliest effort came at the a...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>{He was not only the opponent of the Hungarian...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>{References, In Aghmat, in the year 1190, he w...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>{Life, A Brief View of the Past and Present St...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>{Born in Pikiv, Kalynivka Raion, Vinnytsia Obl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>{References, He won the National Artist award ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         shared_sents  count\n",
       "0   {In fact I won't sell to them., References, Ar...     68\n",
       "1   {The Impaler – المُخوزِق., A historical novel ...     24\n",
       "2   {References, Also a documentary was made about...      7\n",
       "3   {A statue of him, unveiled in 1950, stands at ...     12\n",
       "4   {References, His earliest effort came at the a...     27\n",
       "..                                                ...    ...\n",
       "95  {He was not only the opponent of the Hungarian...     26\n",
       "96  {References, In Aghmat, in the year 1190, he w...     10\n",
       "97  {Life, A Brief View of the Past and Present St...     12\n",
       "98  {Born in Pikiv, Kalynivka Raion, Vinnytsia Obl...      7\n",
       "99  {References, He won the National Artist award ...     17\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of unique sentences for each article for both Spacy and Stanza.\n",
    "# Find the intersection = these are sentences recognized by both libraries.\n",
    "\n",
    "shared_sents_per_article = list(map(lambda x,y: set(x).intersection(set(y)), spacy_sents, stanza_sents))\n",
    "\n",
    "shared_sents_per_article_df = pd.DataFrame(columns=['shared_sents', 'count'])\n",
    "shared_sents_per_article_df['shared_sents'] = shared_sents_per_article\n",
    "shared_sents_per_article_df['count'] = [len(x) for x in shared_sents_per_article]\n",
    "shared_sents_per_article_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the overall count of shared sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Spacy sentences: 3077\n",
      "    Total Stanza sentences: 2823\n",
      "    Shared sentences: 2263\n",
      "    Percentage of shared sentences for Spacy: 73.55%\n",
      "    Percentage of shared sentences for Stanza: 80.16%\n"
     ]
    }
   ],
   "source": [
    "total_sp_sent = sents_df.sum()['Spacy']\n",
    "total_st_sent = sents_df.sum()['Stanza']\n",
    "total_shared = shared_sents_per_article_df['count'].sum()\n",
    "\n",
    "print(f\"\"\"\n",
    "    Total Spacy sentences: {total_sp_sent}\n",
    "    Total Stanza sentences: {total_st_sent}\n",
    "    Shared sentences: {total_shared}\n",
    "    Percentage of shared sentences for Spacy: {100*total_shared/total_sp_sent:.2f}%\n",
    "    Percentage of shared sentences for Stanza: {100*total_shared/total_st_sent:.2f}%\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at what different articles both libraries found looking at the first mismatched sentence pair \n",
    "for each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: 0\n",
      "\n",
      "\n",
      "Spacy sentence: She danced also in Hobart's Opera And Ballet Festival for International Week in 1945.Intending to develop land and to pursue their artistic interests, they moved to Mungo Brush in the Myall Lakes, New South Wales, in 1951 or 1954, living a subsistence existence from prawn fishing and trading their home-grown produce, and were appointed Honorary Rangers there in 1955 under the Wild Flowers and Native Plants Protection Act. \n",
      "\n",
      "Stanza sentence: She danced also in Hobart's Opera And Ballet Festival for International Week in 1945.\n",
      "_______________\n",
      "Article: 1\n",
      "\n",
      "\n",
      "Spacy sentence: ISBN 9782844096265, \n",
      "\n",
      "Stanza sentence: ISBN 9782844096265, ASIN B07TTX6ZHQ\n",
      "_______________\n",
      "Article: 2\n",
      "\n",
      "\n",
      "Spacy sentence: He successfully underwent a ground-breaking bilateral arm transplant in August 2016.Peck wrote a book, Rebuilding Sergeant Peck: How I Put Body and Soul Back Together After Afghanistan, that was released on May 7, 2019. \n",
      "\n",
      "Stanza sentence: He successfully underwent a ground-breaking bilateral arm transplant in August 2016.\n",
      "_______________\n",
      "Article: 3\n",
      "\n",
      "\n",
      "Spacy sentence: Life He was a son of Julius Sabbe and the eldest of seven children. \n",
      "\n",
      "Stanza sentence: Life\n",
      "_______________\n",
      "Article: 4\n",
      "\n",
      "\n",
      "Spacy sentence: In his professional life he has appeared many times for the Scottish Journalists' Eleven and was once 'Man of the Match' at Wembley when his team triumphed over their English counterparts. \n",
      "\n",
      "Stanza sentence: In his professional life he has appeared many times for the Scottish Journalists'\n",
      "_______________\n",
      "Article: 5\n",
      "\n",
      "\n",
      "Spacy sentence: Her latest book, A Colony of Strangers: The founding and early history of Clifden, was published in 2012. \n",
      "\n",
      "Stanza sentence: Her latest book, A Colony of Strangers:\n",
      "_______________\n",
      "Article: 6\n",
      "\n",
      "\n",
      "Spacy sentence: Career As a writer, Cruz has received recognition for his contributions to Philippine literature, including more than thirty books. \n",
      "\n",
      "Stanza sentence: Career\n",
      "_______________\n",
      "Article: 7\n",
      "\n",
      "\n",
      "Spacy sentence: Given its generally disappointing results, the body declined in importance, although Threlfall remained its Secretary until it was wound up, in 1895.Threlfall was subsequently appointed as a magistrate in Southport. \n",
      "\n",
      "Stanza sentence: Given its generally disappointing results, the body declined in importance, although Threlfall remained its Secretary until it was wound up, in 1895.\n",
      "_______________\n",
      "Article: 8\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 9\n",
      "\n",
      "\n",
      "Spacy sentence: Separated from Dave in a bustling street, she fell and twisted her ankle, only to be swept up by a young handsome, chivalrous Egyptian. \n",
      "\n",
      "Stanza sentence: Separated from\n",
      "_______________\n",
      "Article: 10\n",
      "\n",
      "\n",
      "Spacy sentence: Works in English From Wŏnso Pond (Feminist Press 2009) \n",
      "\n",
      "Stanza sentence: Works in English From Wŏnso Pond (Feminist Press 2009) ISBN 978-1-55861-601-1\n",
      "_______________\n",
      "Article: 11\n",
      "\n",
      "\n",
      "Spacy sentence: Family During this tour, White met her future husband, Bruce M. White, a mine owner and developer. \n",
      "\n",
      "Stanza sentence: Family During this tour,\n",
      "_______________\n",
      "Article: 12\n",
      "\n",
      "\n",
      "Spacy sentence: Life The son of Richard Owen, a jeweller of Old Street, London, he entered St Paul's School on 18 October 1777. \n",
      "\n",
      "Stanza sentence: Life\n",
      "_______________\n",
      "Article: 13\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 14\n",
      "\n",
      "\n",
      "Spacy sentence: Works \"Critical study: Problems of the secret self in the Saudi women's novels (1999–2012)\" (Arabic title: I'shkaliyat Al-that Al-saridah \n",
      "\n",
      "Stanza sentence: Works \"Critical study: Problems of the secret self in the Saudi women's novels (1999–2012)\" (Arabic title: I'shkaliyat Al-that Al-saridah fe Al-riwayat Al-nisa'eyah Al-saudiyah), published by Arab Scientific Publishers in 2020, and the book \"The phenomenon of leaving in the Saudi short story- an artistic study\" (Arabic title: Thahirat Al-raheal fe Al-Qisah Al-Qasirah fi Al-mamlakah Al-Arabia Al-Saudiyah- Dirasah Faniyah), published by Al-Jouf Literary Club and Arab Diffusion Company in 2013.\n",
      "_______________\n",
      "Article: 15\n",
      "\n",
      "\n",
      "Spacy sentence: Writings Dust Owuor's 2014 novel Dust portrays the violent history of Kenya in the second half of the 20th century. \n",
      "\n",
      "Stanza sentence: Writings Dust Owuor's 2014 novel\n",
      "_______________\n",
      "Article: 16\n",
      "\n",
      "\n",
      "Spacy sentence: Background and education She was born in Mparo in present-day Rukiga District, in the Western Region of Uganda. \n",
      "\n",
      "Stanza sentence: Background and education\n",
      "_______________\n",
      "Article: 17\n",
      "\n",
      "\n",
      "Spacy sentence: Views In one profile, her views were described as follows: “The political figure Andrews most admires is Marine Le Pen, the leader of France's far-right National Front party, but Trump is not far behind in her estimation. \n",
      "\n",
      "Stanza sentence: Views\n",
      "_______________\n",
      "Article: 18\n",
      "\n",
      "\n",
      "Spacy sentence: Among his works, The Merchant of Joseon (Sangdo, 상도) and Emperor of The Sea (Haeshin, 해신) were dramatized and aired by MBC and KBS in 2001 and 2004, respectively, which won popularity not only among Koreans but also viewers across the globe. \n",
      "\n",
      "Stanza sentence: Among his works,\n",
      "_______________\n",
      "Article: 19\n",
      "\n",
      "\n",
      "Spacy sentence: Career He was also the faculty of the Quaid-e-Azam University, Islamabad. \n",
      "\n",
      "Stanza sentence: Career\n",
      "_______________\n",
      "Article: 20\n",
      "\n",
      "\n",
      "Spacy sentence: Bibliography Gītināṭẏa, Gītābhinaẏa and Suāṅga Raṅgasabhā gītināṭẏa Naḷadamaẏantī gītināṭẏa Rābaṇa badha gītināṭẏa Bakāsura badha gītināṭẏa Bakāsura badha gītināṭẏa Duryẏōdhana badha gītināṭẏa Brajalīḷā suāṅga Dhruba charita suāṅga Kīcakabadha suāṅga \n",
      "\n",
      "Stanza sentence: Bibliography Gītināṭẏa, Gītābhinaẏa and Suāṅga Raṅgasabhā gītināṭẏa Naḷadamaẏantī gītināṭẏa Rābaṇa badha gītināṭẏa Bakāsura badha gītināṭẏa Bakāsura badha gītināṭẏa Duryẏōdhana badha gītināṭẏa Brajalīḷā suāṅga Dhruba charita suāṅga Kīcakabadha suāṅga Dānabīra hariścandra suāṅga Sābitrī satẏabāna suāṅga Nikuñca miḷana suāṅga Pārbatī bibāha suāṅga Raghu arakṣita nābakēḷi suāṅga Muktācōri suāṅga Harapārbatī bibhā suāṅga Prahallāda carita gītābhinaẏa Dāṇḍiparba gītābhinaẏa Lakṣmī pūjā gītābhinaẏa Mahiṣāsura badha Sahasrā rābaṇa badha Bālẏalīḷā Subhadrā haraṇa Mādhaba sulōcanā Srībatsa rājā bā śani lakṣmī kaḷi Jarāsandha badha References\n",
      "_______________\n",
      "Article: 21\n",
      "\n",
      "\n",
      "Spacy sentence: Early career After completing house jobs \n",
      "\n",
      "Stanza sentence: Early career\n",
      "_______________\n",
      "Article: 22\n",
      "\n",
      "\n",
      "Spacy sentence: Leaving the Liberal Party He let his Liberal Party membership lapse in 2002 and in particular became critical of what he said was a takeover of the conservative faction by NSW Member of the Legislative Council David Clarke. \n",
      "\n",
      "Stanza sentence: Leaving the Liberal Party\n",
      "_______________\n",
      "Article: 23\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 24\n",
      "\n",
      "\n",
      "Spacy sentence: He also edited Volume 4 entitled Zoology (1964).Specht worked as a senior research fellow at the University of Adelaide from 1950 while he worked on his MSc. \n",
      "\n",
      "Stanza sentence: He also edited Volume 4 entitled Zoology (1964).\n",
      "_______________\n",
      "Article: 25\n",
      "\n",
      "\n",
      "Spacy sentence: Life Francis Whishaw was born 13 July 1804, the son of John Whishaw, a solicitor. \n",
      "\n",
      "Stanza sentence: Life Francis\n",
      "_______________\n",
      "Article: 26\n",
      "\n",
      "\n",
      "Spacy sentence: He is a son of Ch. \n",
      "\n",
      "Stanza sentence: He is a son of Ch. Fazal Din.\n",
      "_______________\n",
      "Article: 27\n",
      "\n",
      "\n",
      "Spacy sentence: Journalist The couple had four children, but motherhood did not prevent Pamela flourishing in journalism. \n",
      "\n",
      "Stanza sentence: Journalist\n",
      "_______________\n",
      "Article: 28\n",
      "\n",
      "\n",
      "Spacy sentence: Works Philip Nichols, The copie of a letter sente to one maister Chrispyne (London: John Daye, 1548). \n",
      "\n",
      "Stanza sentence: Works Philip Nichols,\n",
      "_______________\n",
      "Article: 29\n",
      "\n",
      "\n",
      "Spacy sentence: Sin Chaeho, or Shin Chae-ho (Korean: 신채호; December 8, 1880 – February 21, 1936), was a Korean independence activist, historian, anarchist, nationalist, and a founder of Korean nationalist historiography (민족 사학, minjok sahak; sometimes shortened to minjok). \n",
      "\n",
      "Stanza sentence: Sin Chaeho, or Shin Chae-ho (Korean: 신채호; December 8, 1880 – February 21, 1936), was a Korean independence activist, historian, anarchist, nationalist, and a founder of Korean nationalist historiography (민족 사학, minjok sahak; sometimes shortened to minjok).: 7 : 27 : 52\n",
      "_______________\n",
      "Article: 30\n",
      "\n",
      "\n",
      "Spacy sentence: Writing De Hamel won the 1979 Esther Glen Award for Take the Long Path (1978), and the 1985 A.W. Reed Memorial Award for Hemi's Pet (1985).Other books written include: X Marks the Spot (1973) \n",
      "\n",
      "Stanza sentence: Writing De Hamel won the 1979 Esther Glen Award for Take the Long Path (1978), and the 1985 A.W. Reed Memorial Award for Hemi's Pet (1985).\n",
      "_______________\n",
      "Article: 31\n",
      "\n",
      "\n",
      "Spacy sentence: Colorado (2005–2013) In 2006, Pevec and Ian Bates developed an educational garden for 90 learning disabled students at the Facilitated Flatirons Academy in Lafayette. \n",
      "\n",
      "Stanza sentence: Colorado (2005–2013)\n",
      "_______________\n",
      "Article: 32\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 33\n",
      "\n",
      "\n",
      "Spacy sentence: Awards In 1999, he received the Arbeitsstipendium des Deutschen Literaturfonds and the Leonce-und-Lena-Preis. \n",
      "\n",
      "Stanza sentence: Awards\n",
      "_______________\n",
      "Article: 34\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 35\n",
      "\n",
      "\n",
      "Spacy sentence: Ilyas Hussain Ibrahim (Dhivehi: އައްޝައިހް އިލިޔާސް ޙުސައިން އިބްރާހީމް, born 5 \n",
      "\n",
      "Stanza sentence: Ilyas Hussain Ibrahim (Dhivehi: އައްޝައިހް އިލިޔާސް ޙުސައިން އިބްރާހީމް, born 5 May 1957) is a Maldivian politician.\n",
      "_______________\n",
      "Article: 36\n",
      "\n",
      "\n",
      "Spacy sentence: First Exile He went into exile in 1920 following the final defeat of the White Army in the Crimea and was elected ataman of the Kuban Host on the Greek island of Lemnos in the Aegean Sea. \n",
      "\n",
      "Stanza sentence: First Exile\n",
      "_______________\n",
      "Article: 37\n",
      "\n",
      "\n",
      "Spacy sentence: Journalist She started reporting in Oviedo, where she moved after separating from her husband. \n",
      "\n",
      "Stanza sentence: Journalist\n",
      "_______________\n",
      "Article: 38\n",
      "\n",
      "\n",
      "Spacy sentence: Career He pursued doctorate in Hindi-language and obtained master's degree in Philosophy, Sanskrit, Prakrit and English from the Karnatak University in Dharwad. \n",
      "\n",
      "Stanza sentence: Career\n",
      "_______________\n",
      "Article: 39\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 40\n",
      "\n",
      "\n",
      "Spacy sentence: Explaining how she came about the name with Jennifer Emelife, she explained: \"The brittleness of paper evokes the ephemeral nature of literary work and ideas within the digital space... \n",
      "\n",
      "Stanza sentence: Explaining how she came about the name with Jennifer Emelife, she explained: \"The brittleness of paper evokes the ephemeral nature of literary work and ideas within the digital space...Brittle Paper is about documenting the life of texts within the social media space.\"\n",
      "_______________\n",
      "Article: 41\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 42\n",
      "\n",
      "\n",
      "Spacy sentence: 1980: \n",
      "\n",
      "Stanza sentence: 1980: Sendisveinninn er einmana, poetry (Gallerí Suðurgata 7, Reykjavík) 1980: Er nokkur í Kórónafötum hér inni?, poetry (Gallerí Suðurgata 7, Reykjavík) 1981: Róbinson Krúsó snýr aftur, poetry (Iðunn, Reykjavík) 1982: Riddarar hringstigans, novel (Almenna bókafélagið, Reykjavík) 1983: Vængjasláttur í þakrennum, novel (Almenna bókafélagið, Reykjavík) 1986: Eftirmáli regndropanna, novel (Almenna bókafélagið, Reykjavík) 1988: Leitin að dýragarðinum, short stories (Almenna bókafélagið, Reykjavík) 1990: Rauðir dagar, novel (Almenna bókafélagið, Reykjavík) 1991: Klettur í hafi, poetry (Almenna bókafélagið, Reykjavík) 1992: Fólkið í steininum, children's book (Almenna bókafélagið, Reykjavík) 1993: Hundakexið, children's book (Almenna bókafélagið, Reykjavík) 1993: Englar alheimsins, novel (Almenna bókafélagið, Reykjavík) 1995: Í augu óreiðunnar: ljóð eða eitthvað í þá áttina, poetry (Mál og menning, Reykjavík) 1995: Ljóð 1980–1981, poetry (Mál og menning, Reykjavík) 1997: Fótspor á himnum, novel (Mál og menning, Reykjavík) 2000: Draumar á jörðu, novel (Mál og menning, Reykjavík) 2001: Kannski er pósturinn svangur, short stories (Mál og menning, Reykjavík) 2002: Ljóð 1980–1995, poetry (Mál og menning, Reykjavík) 2002: Nafnlausir vegir, novel (Mál og menning, Reykjavik) 2004: Bítlaávarpið, novel (Mál og menning, Reykjavík) 2006: Ég stytti mér leið framhjá dauðanum, poetry (Mál og menning, Reykjavík) 2007: Rimlar hugans, novel (Mál og menning, Reykjavík) 2009: Hvíta bókin (The White Book), essays (Mál og menning, Reykjavík) 2011: Bankastræti núll (0 Bank Street), essays (Mál og menning, Reykjavík) 2012: Íslenskir kóngar (Icelandic Kings), novel (Mál og menning, Reykjavík) 2015: Hundadagar (Dog-Days), novel (Mál og menning, Reykjavík) Awards and honors 1995: The Nordic Council's Literature Prize for Englar alheimsins 1999: Karen Blixen medaljen from the Danish Academy 2002: Riddarakross of Hin íslenska fálkaorða fyrir framlag til íslenskra bókmennta 2012: Swedish Academy's Nordic Prize for services to Nordic literature 2015: Íslensku bókmenntaverðlaunin for his novel Hundadagar German language editions Die Ritter der runden Treppe (Riddarar hringstigans, 1982), Munich, Goldmann btb, 1999, ISBN 978-3-442-72495-6 Engel des Universums (Englar alheimsins, 1993), Munich, Goldmann, Bd. 72514 btb, 2000, ISBN 978-3-442-72514-4 Fußspuren am Himmel (Fótspor á himnum, 1997), Munich, Wien, Carl Hanser, 2001, ISBN 978-3-446-20051-7 References External links Edda.is Bokmenntir Einar Már Guðmundsson in the German National Library catalogue – Einar Már Guðmundsson (isl./engl.)\n",
      "_______________\n",
      "Article: 43\n",
      "\n",
      "\n",
      "Spacy sentence: This was celebrated by some and criticized by others, and references was made to when Athena Farrokhzad was criticized for playing the same song on the show in 2014.Geijer has hosted survival courses at ABF were a zombie apocalypse was used as a scenario for survival. \n",
      "\n",
      "Stanza sentence: This was celebrated by some and criticized by others, and references was made to when Athena Farrokhzad was criticized for playing the same song on the show in 2014.\n",
      "_______________\n",
      "Article: 44\n",
      "\n",
      "\n",
      "Spacy sentence: ISBN 9781949641035 Lion cross point (獅子渡り鼻, Shishiwataribana), translated by Angus Turvill, Two Lines Press, 2018. \n",
      "\n",
      "Stanza sentence: ISBN 9781949641035\n",
      "_______________\n",
      "Article: 45\n",
      "\n",
      "\n",
      "Spacy sentence: References The first version of the article is translated and is based from the article at the Greek Wikipedia (el:Main Page) \n",
      "\n",
      "Stanza sentence: References\n",
      "_______________\n",
      "Article: 46\n",
      "\n",
      "\n",
      "Spacy sentence: Works Komt een vrouw bij de dokter (Love Life) (2003) Help, ik heb mijn vrouw zwanger gemaakt! \n",
      "\n",
      "Stanza sentence: Works\n",
      "_______________\n",
      "Article: 47\n",
      "\n",
      "\n",
      "Spacy sentence: Filmography As producer: Konobarica (\"The Waitress\") (1995) \n",
      "\n",
      "Stanza sentence: Filmography\n",
      "_______________\n",
      "Article: 48\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 49\n",
      "\n",
      "\n",
      "Spacy sentence: He is also referred to by the name En no Gyōja (役行者, \"En the ascetic\"), En no Ubasoku (役優婆塞, \"En the Layman\") , or also under the full name \n",
      "\n",
      "Stanza sentence: He is also referred to by the name En no Gyōja (役行者, \"En the ascetic\"), En no Ubasoku (役優婆塞, \"En the Layman\") , or also under the full name En no Kimi Ozunu, where Kimi (君) is his kabane or titular name.\n",
      "_______________\n",
      "Article: 50\n",
      "\n",
      "\n",
      "Spacy sentence: Works His published novels include: Cry Among Rainclouds (2001) \n",
      "\n",
      "Stanza sentence: Works\n",
      "_______________\n",
      "Article: 51\n",
      "\n",
      "\n",
      "Spacy sentence: He worked for the South China Morning Post as a humor columnist known as Lai See until 1997.He has been previously described as an \"outspoken critic of China\". \n",
      "\n",
      "Stanza sentence: He worked for the South China Morning Post as a humor columnist known as Lai See until 1997.\n",
      "_______________\n",
      "Article: 52\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 53\n",
      "\n",
      "\n",
      "Spacy sentence: Education Doctor of Philosophy (Michigan State University, 1986) \n",
      "\n",
      "Stanza sentence: Education Doctor of Philosophy (Michigan State University, 1986) Career Chairman of Alumni Association of Yogyakarta State University (2014–2018) Vice Chairman of Majelis Pendidikan Tinggi Pimpinan Pusat Muhammadiyah (2000–2015) Acting Director General for Primary Education in Indonesian Ministry of Education and Culture (2010–2013) Director General for Management of Primary and Secondary Education in Indonesian Ministry of Education and Culture (2005–2010) Rector of Yogyakarta State University (1999–2006) Secretary of Yogyakarta State University Postgraduate Program (1997–1999) Consultant of Asia Development Bank (ADB) (1997–1998) Consultant of World Bank (1994, 1996) Professor in Yogyakarta State University Publications Books Buku Panduan Media Pembelajaran Literasi Keuangan Robot Need & Want Betapa Mudah Menyusun Tulisan Ilmiah (Esensi, 2016) Menjadi Guru Profesional: Strategi Meningkatkan Kualifikasi dan Kualitas Guru di Era Global (Esensi, 2013) Bagaimana Menjadi Calon Guru dan Guru Profesional (Multi Pressindo, 2013) Wajib Belajar 9 Tahun untuk Masa Depan yang Lebih Baik (Ditjen Dikdas Kemdikbud RI, 2013) Hidup Mati RSBI: Boleh Bubar, Virus Kualitasnya Tetap Menyabar (Ditjen Dikdas Kemdikbud RI, 2013) Bantuan Siswa Miskin: Strategi Jitu Menyukseskan Wajib Belajar 9 Tahun di Indonesia (Ditjen Dikdas Kemdikbud RI, 2013) Betapa Mudah Menulis Karya Ilmiah (Eduka, 2009) Dialog Interaktif tentang Pendidikan: dari Konseptual, Menggelitik, sampai yang Ringan dan Ringan Sekali (Multi Pressindo, 2008) Dinamika Pendidikan Nasional dalam Percaturan Dunia Global (PSAP Muhammadiyah, 2006) Wajah dan Dinamika Pendidikan Anak Bangsa (Adicita Karya Nusa, 2001) Refleksi dan Reformasi Pendidikan di Indonesia Memasuki Milenium III (Adicita Karya Nusa, 2000) Pokok-Pokok Pembelajaran Pendidikan Ekonomi di SLTP (Departemen Pendidikan Nasional, 2000) Belajar: Perkembangan Teori dan Kegiatannya (Yayasan Penerbit FKIS-IKIP Yogyakarta, 1983) Video Video Pembelajaran Literasi Keuangan Need and Want International publications Teachers’ Burnout: A SEM Analysis in an Asian Context (Heliyon, Volume 6 Issue 1 January 2020) Human Resource Management for Improving Internationalization at a Private University in Yogyakarta, Indonesia (Mediterranean Journal of Social Sciences, Vol. 10 No. 2 March 2019, Indexed by SCOPUS) Bringing Voluntary Financial Education in Emerging Economy: Role of Financial Socialization During Elementary Years (The Asia-Pacific Education Researcher, Volume 22/2013 - Volume 26/2017, Indexed by ISI-Thomson) Evidence of Private Wage Returns to Schooling in Indonesia from Labor Force Surveys (Actual Problems of economics No. #2 (188), 2017, Indexed by Scopus) Indonesia’s School Operational Assistance Program (BOS): Challenges in Embedding Results-Based Approach (Seoul, 22–23 Oktober 2012) Enriching Future Generation: Education Promoting Indonesian Self-Development (Yogyakarta; UNY dan Yale University, 27 Juni 2011) Technical and Vocational School Development Strategy in Indonesia (Istanbul, 18 – 20 Juni 2009) Bridging The Education Gap: Improving Access, Equity, And Quality (The Case Of Indonesia) (Kuala Lumpur, 13–14 Maret 2008) Regional Perspective on Current Initiatives and Opportunities for E-9 Teacher Networking: Indonesia – China (Seventh E-9 Ministerial Review Meeting, 10–12 Maret 2008) Road Map 2006 – 2010: Policies in the Development of Technical Vocational School in Indonesia (Hanoi, 12 Januari 2008) Primary and Secondary School Management: Challenges and Opportunities (Jakarta, 30 Juli 2007) Indonesian Education in Comparison Of South East Asian Countries (Yogyakarta, 10 December 2004) Strategy of Sport Development within the Frame Work of Local Autonomy: The Case of Indonesia (Yogyakarta, 10 September 2003) Dialogue Among the Civilizations: The Role of Universities, Country Perspective: Indonesia (Korea, 20 – 23 November 2002) A Glance at the Indonesian Educational System (Yogyakarta, 5-6 Februari 2002) Education for Tolerance and Human Rights: Building Socio-Pedagogical Models for Indonesian Harmony in Diversity (Yogyakarta, 16 Juli 2001) National Education Reform Agenda: The Principles of Change (Jakarta, 5 Juli 2001) National publications Dampak Bantuan Operasional Sekolah (BOS) di Madrasah Tsanawiyah Award Joon S. Moon Distinguished International Alumni Footnotes External links (in English) Alumni Professional Development Activity – Malang, Australiaawardsindonesia.org (in Indonesian) Prof. Suyanto Ph.D.: Negara Takkan Bangkrut untuk Pendidikan, Suaramerdeka.com (in Indonesian) Dirjen Manajemen Pendidikan Dasar dan Menengah Serahkan Dana Darurat Rehabilitasi Sekolah Korban Merapi, Slemankab.go.id (in Indonesian) Arah Kebijakan Pendidikan Indonesia pada Era MEA, Fe.uny.ac.id (in Indonesian) Penyelenggaraan Sekolah Pasca Berlakunya UU Badan Hukum Pendidikan, Dikdas.kemdikbud.go.id (in Indonesian) Peluncuran Buku Prof. Suyanto, harian Suara Merdeka edisi 26 September 2006. (in Indonesian) Integritas Pendidikan, Ispi.or.id\n",
      "_______________\n",
      "Article: 54\n",
      "\n",
      "\n",
      "Spacy sentence: Later years His public career was marked by great independence and fidelity to principle. \n",
      "\n",
      "Stanza sentence: Later years\n",
      "_______________\n",
      "Article: 55\n",
      "\n",
      "\n",
      "Spacy sentence: Works U \n",
      "\n",
      "Stanza sentence: Works U ime oca i sina, 1969 (novel) Povijest izgubljene duše, 1980 (novel) Braća, 1983 (novel) Topot divljih konja, 1989 (novel) Osmjehni se i u plaču, 2000 (novel) Središte, 1973 (short story) Živjeti smrt: sarajevski dnevnik, 1996 Treći svjetski rat, 1999 References\n",
      "_______________\n",
      "Article: 56\n",
      "\n",
      "\n",
      "Spacy sentence: Youthful activism After graduating from the lower high school in Ptuj, he enrolled in the classical gymnasium in Maribor; he was the first generation of students who took their courses entirely in Slovene (before that, courses were still partially taught in German). \n",
      "\n",
      "Stanza sentence: Youthful activism\n",
      "_______________\n",
      "Article: 57\n",
      "\n",
      "\n",
      "Spacy sentence: In a break from Morocco between 1974 and 1978 to pursue his painting career, Hamri published his Tales of Joujouka. 1990s to 2000 On his return to Morocco Hamri built a new house in Zahjouka, which became a gathering place for the musicians. \n",
      "\n",
      "Stanza sentence: In a break from Morocco between 1974 and 1978 to pursue his painting career, Hamri published his Tales of Joujouka.\n",
      "_______________\n",
      "Article: 58\n",
      "\n",
      "\n",
      "Spacy sentence: Maputo, Associação dos Escritores Moçambicanos, 1989 \n",
      "\n",
      "Stanza sentence: Maputo, Associação dos Escritores Moçambicanos, 1989 A Ilha e Moçambique pela voz dos poetas.\n",
      "_______________\n",
      "Article: 59\n",
      "\n",
      "\n",
      "Spacy sentence: Kirkus Reviews characterized her 2009 first novel, The Opposite of Love, as a \"proposed merger of literary fiction with chick lit \n",
      "\n",
      "Stanza sentence: Kirkus Reviews characterized her 2009 first novel, The Opposite of Love, as a \"proposed merger of literary fiction with chick lit [that] contravenes the conventions of both genres.\"\n",
      "_______________\n",
      "Article: 60\n",
      "\n",
      "\n",
      "Spacy sentence: Early life Ghosh was born in Hat Gopalpur village in the Jessore district in undivided Bengal, (presently Bangladesh), on 20 June 1923.Due to poverty, Ghosh could not continue his education further and had to become a professional soon after. \n",
      "\n",
      "Stanza sentence: Early life Ghosh was born in Hat Gopalpur village in the Jessore district in undivided Bengal, (presently Bangladesh), on 20 June 1923.\n",
      "_______________\n",
      "Article: 61\n",
      "\n",
      "\n",
      "Spacy sentence: Cultural influences In his 1993 play Hysteria, British playwright Terry Johnson created a character partly based on Yahuda's attempt to convince Sigmund Freud not to publish his final book, Moses and Monotheism. \n",
      "\n",
      "Stanza sentence: Cultural influences\n",
      "_______________\n",
      "Article: 62\n",
      "\n",
      "\n",
      "Spacy sentence: Diplomacy For five and a half years, 2001–2006, Kurspahić worked for the United Nations Office on Drugs and Crime, first as the spokesman in Vienna, Austria and then as the Caribbean Regional Representative in Barbados, covering 29 states and territories. \n",
      "\n",
      "Stanza sentence: Diplomacy\n",
      "_______________\n",
      "Article: 63\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 64\n",
      "\n",
      "\n",
      "Spacy sentence: Biography He attended high school in Knin and Split. \n",
      "\n",
      "Stanza sentence: Biography\n",
      "_______________\n",
      "Article: 65\n",
      "\n",
      "\n",
      "Spacy sentence: Works Books (ed.) \n",
      "\n",
      "Stanza sentence: Works Books (ed.) Letters to Leigh Hunt from his son Vincent (Cloanthus Press, 1934) (with Desmond Flower) English Poetical Autographs (Cassell, 1938) \"Some Caricatures of Book-Collectors – An Essay\" (printed for private circulation by William H. Robinson Ltd, Christmas 1948) The Alabaster Hand and other Ghost Stories (Dobson, 1949) Phillips Studies, 5 vols.\n",
      "_______________\n",
      "Article: 66\n",
      "\n",
      "\n",
      "Spacy sentence: She held previous appointments in the English Department at Rutgers University–Camden (1998–2002) and Wake Forest University (1997–1998).Levine has published three books of literary criticism: The Serious Pleasures of Suspense: Victorian Realism and Narrative Doubt (2003), which won the Perkins Prize for Best Book in Narrative Studies; Provoking Democracy: Why We Need the Arts (2007); and Forms: Whole, Rhythm, Hierarchy, Network (2015), which was awarded the James Russell Lowell Prize from the Modern Language Association and the Dorothy Lee Award for Outstanding Scholarship in the Ecology of Culture. \n",
      "\n",
      "Stanza sentence: She held previous appointments in the English Department at Rutgers University–Camden (1998–2002) and Wake Forest University (1997–1998).\n",
      "_______________\n",
      "Article: 67\n",
      "\n",
      "\n",
      "Spacy sentence: Works A Gift Horse, and Other Stories (1977), Poolbeg Press The Homesick Garden (1991), Poolbeg Press If Only: Short Stories of Love and Divorce (1997), Poolbeg Press (co-editor) \n",
      "\n",
      "Stanza sentence: Works A Gift Horse, and Other Stories (1977), Poolbeg Press The Homesick Garden (1991), Poolbeg Press If Only: Short Stories of Love and Divorce (1997), Poolbeg Press (co-editor) In Sunshine Or in Shadow (1999), Poolbeg Press (co-editor) References External links Kate Cruise O'Brien at Find a Grave\n",
      "_______________\n",
      "Article: 68\n",
      "\n",
      "\n",
      "Spacy sentence: Gmeiner-Verlag 2010, ISBN 978-3905881097 Sinfonie des Todes. \n",
      "\n",
      "Stanza sentence: Gmeiner-Verlag 2010, ISBN 978-3905881097 Sinfonie des Todes. Historischer Kriminalroman.\n",
      "_______________\n",
      "Article: 69\n",
      "\n",
      "\n",
      "Spacy sentence: Geopolitical Wished of Third Rome, 1995 New Democratic Russian Tsars and Turkish Reality, 1994 Azerbaijan on the Path of Total Independence, 1992 Azerbaijan`s Struggle for Independence, 1992 Today Panturkism and Panislamism in Azerbaijan, 1990 Soul Azerbaijan, 1990Translations: The Way Leading to the Bloody Saturday Night in Baku, Bakhtiyar Vahapzadeh, 1990 \n",
      "\n",
      "Stanza sentence: Geopolitical Wished of Third Rome, 1995 New Democratic Russian Tsars and Turkish Reality, 1994 Azerbaijan on the Path of Total Independence, 1992 Azerbaijan`s Struggle for Independence, 1992 Today Panturkism and Panislamism in Azerbaijan, 1990 Soul Azerbaijan, 1990Translations: The Way Leading to the Bloody Saturday Night in Baku, Bakhtiyar Vahapzadeh, 1990 The Last Drive to the South by Vladimir Jirinosky, 1995Published Articles: Dozens of articles published in English Report on the USSR, Eurasian Studies, Turkish Daily News etc. Headlines of some of the articles Azerbaijani Intellectuals Express Concern over Native Language Azerbaijani Press Discusses Link Between Ecological Problems and Health Defects Environmental Pollution, Infertility and Divorce in Azerbaijan.\n",
      "_______________\n",
      "Article: 70\n",
      "\n",
      "\n",
      "Spacy sentence: Post-revolution He returned home in 1920 and two years later joined the CPSU. \n",
      "\n",
      "Stanza sentence: Post-revolution\n",
      "_______________\n",
      "Article: 71\n",
      "\n",
      "\n",
      "Spacy sentence: Career In June 2002, McGuire wrote one of his first articles for South China Morning Post, titled \"Best foot forward for walks in Macau\". \n",
      "\n",
      "Stanza sentence: Career\n",
      "_______________\n",
      "Article: 72\n",
      "\n",
      "\n",
      "Spacy sentence: Think tanks He was a member of the Wilton Park council (1984-1988) and a member of the advisory board of the Woodrow Wilson Chair of International Politics (1985-1992). \n",
      "\n",
      "Stanza sentence: Think tanks\n",
      "_______________\n",
      "Article: 73\n",
      "\n",
      "\n",
      "Spacy sentence: Gout and its Relations to Diseases of the Liver and Kidneys, 1885; 7th edit. 1894; translated into French from the third edition, Paris, 1887; and into German from the fourth edition, Vienna and Leipzig, 1887. \n",
      "\n",
      "Stanza sentence: Gout and its Relations to Diseases of the Liver and Kidneys, 1885; 7th edit.\n",
      "_______________\n",
      "Article: 74\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 75\n",
      "\n",
      "\n",
      "Spacy sentence: Awards Anannya Top Ten Awards (2005) Ananya Literature Award (2010) \n",
      "\n",
      "Stanza sentence: Awards Anannya Top Ten Awards (2005) Ananya Literature Award (2010) Bangla Academy Literary Award (2016) Works Agunmukhar Meye References\n",
      "_______________\n",
      "Article: 76\n",
      "\n",
      "\n",
      "Spacy sentence: Early life He was born in a cottage near Sutton-in-Ashfield in Sherwood Forest, Nottinghamshire, the son of Samuel Hall, a Quaker cobbler and Eleanor Spencer, a dairymaid. \n",
      "\n",
      "Stanza sentence: Early life\n",
      "_______________\n",
      "Article: 77\n",
      "\n",
      "\n",
      "Spacy sentence: In recognition of her commitment to strengthening ties between the UK and South Africa, she received the \"Diplomat of the year from Africa award\" from The Diplomat magazine in 2009.Mabuza died on 6 December 2021, at the age of 83, having been suffering from cancer. \n",
      "\n",
      "Stanza sentence: In recognition of her commitment to strengthening ties between the UK and South Africa, she received the \"Diplomat of the year from Africa award\" from The Diplomat magazine in 2009.\n",
      "_______________\n",
      "Article: 78\n",
      "\n",
      "\n",
      "Spacy sentence: Publications Some of his works include the followings: \n",
      "\n",
      "Stanza sentence: Publications\n",
      "_______________\n",
      "Article: 79\n",
      "\n",
      "\n",
      "Spacy sentence: His ancestors settled in Karbala more than a thousand years ago, before the battle of Karbala, and had the honour of serving in the Imam Husayn and al-Abbas shrines', as well as holding custodianship of the two shrines.al-Sarraf's great ancestor, Mehdi al-Asadi, was a renowned poet, and upon performing one of his pieces of poetry, instead of saying bezeghet (Arabic: بزغت), he made a speech error, and said bedeqet (Arabic: بدقت), and because of that incident, he became known as Haaj Mehdi Bedget. \n",
      "\n",
      "Stanza sentence: His ancestors settled in Karbala more than a thousand years ago, before the battle of Karbala, and had the honour of serving in the Imam Husayn and al-Abbas shrines', as well as holding custodianship of the two shrines.\n",
      "_______________\n",
      "Article: 80\n",
      "\n",
      "\n",
      "Spacy sentence: In 2020 she presented Inside the Bat Cave, which was broadcast on the BBC.She is the author of The Truth About Animals: Stoned Sloths, Lovelorn Hippos, and Other Tales from the Wild Side of Wildlife, which investigates popular misconceptions about animals, including sloths, hyenas, penguins, and pandas. \n",
      "\n",
      "Stanza sentence: In 2020 she presented Inside the Bat Cave, which was broadcast on the BBC.\n",
      "_______________\n",
      "Article: 81\n",
      "\n",
      "\n",
      "Spacy sentence: When asked by FourFourTwo, Hurrey nominated He Always Puts It to the Right: A History Of The Penalty Kick by Clark Miller (1998) as his favourite ever football book. \n",
      "\n",
      "Stanza sentence: When asked by FourFourTwo, Hurrey nominated He Always Puts\n",
      "_______________\n",
      "Article: 82\n",
      "\n",
      "\n",
      "Spacy sentence: Selected bibliography As author: Great Speeches from Shakespeare's Plays: with Notes and a Life of Shakespeare (1891) \n",
      "\n",
      "Stanza sentence: Selected bibliography\n",
      "_______________\n",
      "Article: 83\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 84\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 85\n",
      "\n",
      "\n",
      "Spacy sentence: Adulthood In 1970, Samatar began working at the National Teaching College in Somalia alongside several American librarians. \n",
      "\n",
      "Stanza sentence: Adulthood\n",
      "_______________\n",
      "Article: 86\n",
      "\n",
      "\n",
      "Spacy sentence: He is known for two other specialist works: The Man Behind the Iron Mask (first published in 1988 and revised several times since), and Turtle Tortoise, Image and Symbol. \n",
      "\n",
      "Stanza sentence: He is known for two other specialist works:\n",
      "_______________\n",
      "Article: 87\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 88\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 89\n",
      "\n",
      "\n",
      "Spacy sentence: Jkvr. \n",
      "\n",
      "Stanza sentence: Jkvr. Cécile Wilhelmina Elisabeth Jeanne Petronella de Jong van Beek en Donk (19 May 1866, in Alkmaar – 5 June 1944, in Méréville) was a Dutch feminist writer.\n",
      "_______________\n",
      "Article: 90\n",
      "\n",
      "\n",
      "Spacy sentence: At his request, it was included in the list of Intangible Cultural Heritage in Austria in 2010.He published dialect poems, radio plays and novels over many decades. \n",
      "\n",
      "Stanza sentence: At his request, it was included in the list of Intangible Cultural Heritage in Austria in 2010.\n",
      "_______________\n",
      "Article: 91\n",
      "\n",
      "\n",
      "Spacy sentence: Published works Sawalief (parables) 2006 \n",
      "\n",
      "Stanza sentence: Published works Sawalief (parables) 2006 Al-Mam’out (The featherless bird) 2008 Awja’ Watan (The pains of the homeland) 2012 Solo Bleeding 2012 Plays\n",
      "_______________\n",
      "Article: 92\n",
      "\n",
      "\n",
      "Spacy sentence: Writing and broadcasting He has written several books in both Irish and English, documenting his travels, and also volumes of fictional short stories. \n",
      "\n",
      "Stanza sentence: Writing and broadcasting\n",
      "_______________\n",
      "Article: 93\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 94\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 95\n",
      "\n",
      "\n",
      "Spacy sentence: Arthur Koestler, at the time a high school pupil in Budapest, recalls Szabó as one of the new teachers brought to his school by the revolutionary regime – \"A shy, soft spoken, somewhat absent-minded man, he told us of a subject more faraway than the Moon: the daily life of hired agricultural workers in the countryside\" Support for the revolution was, however, a brief interlude in Szabó's life, and he soon developed into an outspoken and vehement opponent of the short-lived Hungarian Soviet Republic proclaimed by Béla Kun. \n",
      "\n",
      "Stanza sentence: Arthur Koestler, at the time a high school pupil in Budapest, recalls Szabó as one of the new teachers brought to his school by the revolutionary regime – \"A shy, soft spoken, somewhat absent-minded man, he told us of a subject more faraway than the Moon: the daily life of hired agricultural workers in the countryside\"\n",
      "_______________\n",
      "Article: 96\n",
      "\n",
      "\n",
      "Spacy sentence: Bibliography Editions of Sefer ha-Ner Arranged in reverse chronological order: Sefer ha-Ner al Massekhet Shabbat, S. Eidenson Jerusalem, 2010 Shittah Mekubbetzet Kadmon: sefer ha-ner, massekhet Bava Kamma, ed. \n",
      "\n",
      "Stanza sentence: Bibliography Editions of Sefer ha-Ner Arranged in reverse chronological order: Sefer ha-Ner al Massekhet Shabbat, S. Eidenson Jerusalem, 2010 Shittah Mekubbetzet Kadmon: sefer ha-ner, massekhet Bava Kamma, ed. Dov Zeev Havlin, Jerusalem, Institute for the Complete Israeli Talmud (Yad ha-Rav Herzog) 2009/10, 358 pp Shittah Mekubbetzet ha-Ner, massekhet Bava Batra, Jerusalem, Otzar ha-Poskim 1999 Shittah Mekubbetzet ha-Ner, massekhet Bava Batra, Jerusalem, Machon Harry Fischel 1987/8, ed. Yekutiel Zalman Cohen, 175 pp Sefer ha-Ner al Massekhet Berakhot (ed. Meir David ben-Shem), Jerusalem, Machon Torah Shelemah 1958, 141 pp A digest of commentaries on the tractates Bābhā ķammā, Bābhā meṣīʻā and Bābhā bhātherā of the Babylonian Talmud, ed. Leveen, British Museum 1961 Compilations of commentaries including extracts from Sefer ha-Ner Arranged by order of tractate: Kovetz Rishonim le-Massekhet Moed Katan, ed Nissan Sachs, Institute for the Complete Israeli Talmud 1966 Ohel Yeshayahu, ed Hillel Mann, Bava Kamma, 2000-1 Shittat Ha-Kadmonim, M.Y. Blau, Bava Metzia, Bava Batra (2 volumes) Arba’ah Sefarim Niftahim: Sefer Perushe Rabbenu Hananel u-Vet Medrasho: Bava Batra, ed Cohen, 2002, 544 pp Kadmonim al Massekhet Bava Batra, Jerusalem, Agudat Torat Hesed 2004 Kovetz Sakotah le-Roshi, Bava Batra, Bnei Brak 2003 Hiddushe ha-R”I Migas le-Massekhet Bava Batra, ed. Shapira, Machon Torani-Sifruti Oraita, 1985, 280 pp Hiddushe ha-R”I Migas le-Massekhet Bava Batra, ed. Shapira, Friedman, 1978, 266 pp Secondary literature S. Assaf, \"Chelek miPirush Kadmon le-Massekhet Berachot le-Echad mi-Bnei Zemano shel ha-Rambam\", in Le-Zikhron R' Z. P. Chayyes, Jerusalem 1933 Yehoshua Hutner, \"Sefer ha-Ner le-Rabbenu Zechariah Aghmati\", in Sefer Zikkaron le-R. Yitzchak Yedidyah Frankel, Tel Aviv 1992 (contains first chapter of Bava Kamma) C. Z. Hirschburg, Tarbiẕ 42 (1973) Ta Shma Sifrut Haparshanit pp 156–159 Y. Malchi, \"R. Zechariah Aghmati, ha-Ish, Yetzirato, ha-Parshanut, ve-Yachasah le-Ferushei Rashi\", Shanan 14 (2009) pp 65–73 Y. Malchi, \"Rashi's Commentary to Tractate Berachot Included in Sefer Haner of R. Zecharyah Agamati\" [in Hebrew], Alei Sefer, vol. 17, 1993, p. 85-95.\n",
      "_______________\n",
      "Article: 97\n",
      "\n",
      "\n",
      "Spacy sentence: He became barrister-at-law and graduated LL.D.By royal licence, Jervis-White assumed the name of Jervis in addition to that of White, and was created a baronet of Ireland 10 November 1797, the first of the Jervis-White-Jervis baronets. \n",
      "\n",
      "Stanza sentence: He became barrister-at-law and graduated LL.D.\n",
      "_______________\n",
      "Article: 98\n",
      "\n",
      "\n",
      "Text segmentized into sentences in the same way.\n",
      "_______________\n",
      "Article: 99\n",
      "\n",
      "\n",
      "Spacy sentence: Early life He was born in Khong District, Champasak Province, Laos. \n",
      "\n",
      "Stanza sentence: Early life\n",
      "_______________\n"
     ]
    }
   ],
   "source": [
    "def get_first_mismatched_sent_pair(sp_sents, st_sents):\n",
    "    n = len(sp_sents)\n",
    "    m = len(st_sents)\n",
    "    \n",
    "    # Go through the array of sentences for both Spacy and Stanza till the end of smallest of them.\n",
    "    for i in range(min(n, m)):\n",
    "        # When we reach the first mismatch, print the sentences.\n",
    "        if sp_sents[i] != st_sents[i]:\n",
    "            print(f'Spacy sentence: {sp_sents[i]} \\n\\n'\n",
    "            f'Stanza sentence: {st_sents[i]}\\n'\n",
    "            '_______________')\n",
    "            return sp_sents[i], st_sents[i]\n",
    "    \n",
    "    # If there was no mismatch during the array traversal, then all the sentences were tokenized together.\n",
    "    print('Text segmentized into sentences in the same way.\\n_______________')\n",
    "    return None\n",
    "\n",
    "for i, pair in enumerate(zip(spacy_sents, stanza_sents)):\n",
    "    print(f'Article: {i}\\n\\n')\n",
    "    get_first_mismatched_sent_pair(pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sample we see above we can note the following behavior for sentence tokenization for Spacy and Stanza:\n",
    "- If there is a missing space before a period, separating sentences, Stanza seems to perform sentence segmentation better, e.g `Article 0, 2, 7, 24`...\n",
    "- In our source material some sections have a title which is not punctuated. Stanza appears to recognize such cases as a separate sentence (correct) more frequently, e.g. `Article 3, 6, 12, 16, 17, 19`...\n",
    "- However, Stanza seems to be incorrectly splitting a sentence more frequently if there is a punctuation sign followed by a capital letter, e.g. `Article 4, 5, 9, 11, 15, 18`...\n",
    "- Both libraries struggle (as expected) with sententences that have a lot of non-English words or the input string is not a correct sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can form a list of shared sentences for all articles combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sets into arrays and then flatted the 2 array into 1-d array using numpy.concatenate\n",
    "\n",
    "shared_sents = np.concatenate(list(map(lambda x: list(x), shared_sents_per_article)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"In fact I won't sell to them.\", 'References',\n",
       "       'Artists must create for the sake of creating.',\n",
       "       'Germaine, Max; Bertouch, Anne von (1991).',\n",
       "       'Melbourne: Lansdowne.'], dtype='<U592')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_sents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store the shared sentences into a separate DataFrame and into its own CSV-file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anne Von Bertouch, (29 June 1915 – 31 March 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISBN 978-0-9592824-1-2. OCLC 27623615.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Von Bertouch, Anne (1983).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The terraces were purchased by Dr Dick Lees fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What was it before it was a gallery?.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>In popular culture In 1999, Komphet Phorncharo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>In 1996, Butnakho started to write songs in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>The melody came from the pattern of \"Sao Simue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>In 2007, Isan music became very popular in Lao...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>He has eleven siblings.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence\n",
       "0     Anne Von Bertouch, (29 June 1915 – 31 March 20...\n",
       "1                ISBN 978-0-9592824-1-2. OCLC 27623615.\n",
       "2                            Von Bertouch, Anne (1983).\n",
       "3     The terraces were purchased by Dr Dick Lees fo...\n",
       "4                 What was it before it was a gallery?.\n",
       "...                                                 ...\n",
       "2258  In popular culture In 1999, Komphet Phorncharo...\n",
       "2259  In 1996, Butnakho started to write songs in th...\n",
       "2260  The melody came from the pattern of \"Sao Simue...\n",
       "2261  In 2007, Isan music became very popular in Lao...\n",
       "2262                            He has eleven siblings.\n",
       "\n",
       "[2263 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run to generate the DataFrame for this time\n",
    "\"\"\"\n",
    "shared_sents_df = pd.DataFrame(columns=['Sentence'])\n",
    "shared_sents_df.Sentence = shared_sents\n",
    "\"\"\"\n",
    "\n",
    "shared_sents_df = pd.read_csv('SpacyStanza/spacy_stanza_shared_sents.csv', index_col=0)\n",
    "shared_sents_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment below we will use all the tokens returned by Spacy and Stanza, without filtering any punctuation or spaces.\n",
    "\n",
    "However, prior to all the analysis texts were cleaned: we removed linebreaks, tabulation and special characters like `|`, `^`, `<`, `>`, `+`, and `=`. These symbols were not returned when we requested the original texts.\n",
    "\n",
    "We will do the final step of preprocessing and lowercase the text in order to avoid the same word being in vocabulary twice: for different spelling when it's in the beginning or in the middle of a sentence.\n",
    "\n",
    "For our experiment we will take only the sentences that were previously segmented in the same way by Spacy and Stanza.\n",
    "For **SharedTokensNosentences** we will stich all the sentences together and separate them by spaces. For **SharedTokensInSentences** we will apply tokenization on the sentences from `shared_sents_df` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_sents_lower = [s.lower() for s in shared_sents_df.Sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_sents_stiched = ' '.join(shared_sents_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare vocabularies we will run the comparison on the sentences stiched together into one text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set to find unique tokens only\n",
    "spacy_vocabulary = set(spacy_tokenize_text(shared_sents_stiched, no_filtering=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run for the first time Stanza tokenization:\n",
    "\"\"\" \n",
    "stanza_vocabulary = set(stanza_tokenize_text(shared_sents_stiched))with open('SpacyStanza/no_sent_stanza_tokens.pickle', 'wb') as f:\n",
    "with open('SpacyStanza/stanza_vocabulary.pickle', 'wb') as f:\n",
    "    pickle.dump(stanza_vocabulary, f)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# To load Stanza tokens from a pickle file\n",
    "\n",
    "with open('SpacyStanza/stanza_vocabulary.pickle', 'rb') as f:\n",
    "    stanza_vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy vocabulary size: 8488\n",
      "Stanza vocabulary size: 8526\n"
     ]
    }
   ],
   "source": [
    "print(f'Spacy vocabulary size: {len(spacy_vocabulary)}\\nStanza vocabulary size: {len(stanza_vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared vocabulary is calculated as intersection of sets of Stanza and Spacy vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocab = spacy_vocabulary.intersection(stanza_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocab_size = len(shared_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of shared vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8264"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Please **DO NOT** rerun the cell below</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['moving', 'specialized', 'there', 'ekspres', 'attended', 'pune',\n",
       "       'foot', 'occasion', 'favored', '1961', '1659', 'believed', 'equal',\n",
       "       'warsaw', 'turkish', 'height', 'growing', 'c.', 'having',\n",
       "       'hinting', 'friends', 'sweetheart', 'organically', 'registry',\n",
       "       'lhadj', 'nicholas', 'judgement', 'confinement', 'argentina',\n",
       "       'pleasures', 'armidale', 'bunyan', 'immediate', 'emigrants',\n",
       "       'ṛṛays', 'blessing', 'mae', 'extinct', 'mosques', 'stockholm',\n",
       "       'stanitsa', 'matter', 'muffins', 'ceramics', 'egg', 'saitovic',\n",
       "       'tourism', 'founded', 'hospitalized', 'an'], dtype='<U19')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(list(shared_vocab), size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sample above we can observe that both libraries seem to agree on tokenizing a wide range of words:\n",
    "- Common words like `an`, `egg`, `there`, `having` are present in the shared vocabulary.\n",
    "- Foreign to English words like `bunyan`, `stanitsa`, `lhadj` are also present in the shared vocabulary.\n",
    "- We can as well see some Named Entities like dates and locations\n",
    "- We can as well see an incorrect lemma `ṛṛays` (presumably a part of `arrays`) which was produced by both libraries.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy-only vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'m\",\n",
       " \"'ve\",\n",
       " '):',\n",
       " ',(1898',\n",
       " '/priče',\n",
       " '/zapisi',\n",
       " '0',\n",
       " '040',\n",
       " '09',\n",
       " '1031–1034',\n",
       " '111',\n",
       " '1134068708',\n",
       " '1442252813',\n",
       " '1472806871',\n",
       " '1476679488',\n",
       " '1500',\n",
       " '1547–1564',\n",
       " '1689–1887',\n",
       " '16–17',\n",
       " '1744–1816',\n",
       " '1758',\n",
       " '1766–1822',\n",
       " '1766–1830',\n",
       " '1785–1863',\n",
       " '1793–1869',\n",
       " '1795–1864',\n",
       " '1798–1879',\n",
       " '1804–1856',\n",
       " '1804–1871',\n",
       " '1810–1860',\n",
       " '1848–1905',\n",
       " '1860–1923,1986',\n",
       " '1868–1936',\n",
       " '1877–1951',\n",
       " '1882–1956',\n",
       " '1885–1900',\n",
       " '1904–1920',\n",
       " '1905–1907',\n",
       " '1909–1910',\n",
       " '1909–1919',\n",
       " '1912–1928',\n",
       " '1913–1974',\n",
       " '1918–1923',\n",
       " '1928–1998',\n",
       " '1935–37',\n",
       " '1937–39',\n",
       " '1941–45',\n",
       " '1943–24',\n",
       " '1945–47',\n",
       " '1949–2009',\n",
       " '1950/1951',\n",
       " '1969–70',\n",
       " '1977–1980',\n",
       " '1980–84',\n",
       " '1989–1991',\n",
       " '1990–2008',\n",
       " '1996–2000',\n",
       " '1998–2002',\n",
       " '19–24',\n",
       " '2001–2011',\n",
       " '2005–2006',\n",
       " '2008–13',\n",
       " '2011/2012',\n",
       " '2018edited',\n",
       " '2019–20',\n",
       " '205',\n",
       " '23833',\n",
       " '255',\n",
       " '25–28',\n",
       " '293',\n",
       " '300p',\n",
       " '313',\n",
       " '323',\n",
       " '35–6',\n",
       " '3865',\n",
       " '3905881028',\n",
       " '39–40',\n",
       " '401',\n",
       " '441–2',\n",
       " '480–44',\n",
       " '521',\n",
       " '53',\n",
       " '605',\n",
       " '700–707',\n",
       " '7349',\n",
       " '78442',\n",
       " '7–17',\n",
       " '8084',\n",
       " '8097',\n",
       " '8392',\n",
       " '84.5월호',\n",
       " '905237',\n",
       " '9213',\n",
       " '9223',\n",
       " '9592824',\n",
       " '95–96',\n",
       " '9695188',\n",
       " '975',\n",
       " '976',\n",
       " '978',\n",
       " '9781931883702short',\n",
       " '`',\n",
       " 'a$1.6',\n",
       " 'and/or',\n",
       " 'art;\"i',\n",
       " 'asilah—40',\n",
       " 'awardee',\n",
       " 'b.a',\n",
       " 'b.w',\n",
       " 'baked',\n",
       " 'bello',\n",
       " 'biografie',\n",
       " 'bmj',\n",
       " 'bmj.319.7216.1031',\n",
       " 'brassey',\n",
       " 'c.e',\n",
       " 'canonical',\n",
       " 'cc',\n",
       " 'chan',\n",
       " \"chuch'e\",\n",
       " 'co',\n",
       " 'coca',\n",
       " 'cola',\n",
       " 'colonialism',\n",
       " 'covid-19',\n",
       " 'd.c',\n",
       " 'd.l',\n",
       " 'dc',\n",
       " 'denominational',\n",
       " 'devin',\n",
       " 'dmitrievakain',\n",
       " 'doi:10.1136',\n",
       " 'e',\n",
       " 'eck',\n",
       " 'editors',\n",
       " 'eminent',\n",
       " 'enactment',\n",
       " 'esq',\n",
       " 'ethno',\n",
       " 'f.r.c.p',\n",
       " \"faingata'a\",\n",
       " 'fc',\n",
       " 'feudal',\n",
       " 'fl',\n",
       " 'h.c',\n",
       " 'hada/',\n",
       " 'heiress',\n",
       " 'imperialism',\n",
       " 'inst',\n",
       " 'instrumentalist',\n",
       " 'isl',\n",
       " 'j.d',\n",
       " 'jr',\n",
       " 'ju',\n",
       " 'kei',\n",
       " 'kishinev',\n",
       " 'kwan',\n",
       " 'kyoto',\n",
       " 'l.c',\n",
       " 'l.r.c.p',\n",
       " 'l.r.c.s',\n",
       " 'lansdowne',\n",
       " 'liechtenstein',\n",
       " 'm.a',\n",
       " 'm.d',\n",
       " 'm.r',\n",
       " 'm.r.c.p',\n",
       " 'm.r.c.s',\n",
       " 'madison',\n",
       " 'mishnayot',\n",
       " 'mujeres',\n",
       " 'multi',\n",
       " 'n.s.w',\n",
       " 'neo',\n",
       " 'ner',\n",
       " 'nesanice/',\n",
       " 'non',\n",
       " 'novo',\n",
       " 'obi',\n",
       " 'otto',\n",
       " 'p.j',\n",
       " 'ph.d',\n",
       " 'philosophy-',\n",
       " 'pjesme/',\n",
       " 'pre',\n",
       " 'prof',\n",
       " 're',\n",
       " 's.s',\n",
       " 'semi',\n",
       " 'semite',\n",
       " 'semitic',\n",
       " 'semitism',\n",
       " 'serbo',\n",
       " 'sueñan',\n",
       " 'sun',\n",
       " 'suzuki',\n",
       " 't',\n",
       " 't.c',\n",
       " 'tangier,2004',\n",
       " 'thejournal.ie',\n",
       " 'turn',\n",
       " \"u'uqiank'uuh\",\n",
       " \"u'uqinak'uuh\",\n",
       " 'u.k',\n",
       " 'u.s',\n",
       " 'u.s.specht',\n",
       " 'unesco',\n",
       " 'v.g',\n",
       " 'vatican',\n",
       " 'vol',\n",
       " 'wisconsin',\n",
       " 'x',\n",
       " 'yalıkavak',\n",
       " 'yellow',\n",
       " \"zu'bi\",\n",
       " '¡',\n",
       " '¿',\n",
       " 'χρήστος',\n",
       " 'школ',\n",
       " 'بدقت',\n",
       " 'عرفان',\n",
       " '\\u2009',\n",
       " '\\u200a',\n",
       " '\\u200a '}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_only_vocab = spacy_vocabulary.difference(stanza_vocabulary)\n",
    "spacy_only_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Spacy-only tokens we can see a lot of numbers as well as non-english words (or non-words).\n",
    "\n",
    "However, surprisingly we can also see some more or less common English words:\n",
    "- Some words of relatively more formal register of English: `canonical`, `colonialism`, `denominational`, `editors`, `eminent`, `feudal`, `imperialism`, `instrumentalist`...\n",
    "- Some NE: `kishinev`, `kyoto`, `liechtenstein`, `vatican` (Locations), `suzuki` (Name or Organisation name), `unesco` (Organisation name)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza-only vocubulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'a\",\n",
       " \"'e\",\n",
       " \"'t\",\n",
       " '-1758',\n",
       " '-1980',\n",
       " '-2005',\n",
       " '-3865',\n",
       " '.ie',\n",
       " '/1951',\n",
       " '0-521-23833-1',\n",
       " '1-905237-53',\n",
       " '1031',\n",
       " '1689',\n",
       " '1744',\n",
       " '1766-1822',\n",
       " '1785',\n",
       " '1798',\n",
       " '1810',\n",
       " '1860',\n",
       " '1868',\n",
       " '1882',\n",
       " '1923,1986',\n",
       " '1923-2011',\n",
       " '1943-2012',\n",
       " '313-323',\n",
       " '39',\n",
       " '441',\n",
       " '47',\n",
       " '480',\n",
       " '700',\n",
       " '84',\n",
       " '84.5',\n",
       " '9695188.',\n",
       " '975-293-040-9',\n",
       " '975-293-255-x',\n",
       " '975-8084',\n",
       " '978-0-9592824-1-2.',\n",
       " '978-0-9592824-2-9.',\n",
       " '978-0-9592824-3-6.',\n",
       " '978-1134068708',\n",
       " '978-1442252813',\n",
       " '978-1472806871',\n",
       " '978-1476679488',\n",
       " '978-3-205-78442-5',\n",
       " '978-3-7349-9213-1',\n",
       " '978-3-7349-9223-0',\n",
       " '978-3-8392-1500',\n",
       " '978-3-8392-1978',\n",
       " '978-3-8392-1986-7',\n",
       " '978-3905881028',\n",
       " '978-605-09',\n",
       " '978-605-111-401-9',\n",
       " '978-976-8097-13-2.',\n",
       " '9781931883702',\n",
       " '?.',\n",
       " '[...]',\n",
       " '`the',\n",
       " '`yellow',\n",
       " \"abc's\",\n",
       " 'ah.',\n",
       " 'anc.',\n",
       " 'anti-apartheid',\n",
       " 'anti-colonialism',\n",
       " 'anti-feudal',\n",
       " 'anti-german',\n",
       " 'anti-imperialism',\n",
       " 'anti-love',\n",
       " 'anti-semite',\n",
       " 'anti-semitic',\n",
       " 'anti-semitism',\n",
       " 'anti-soviet',\n",
       " 'anti-vatican',\n",
       " 'april–june',\n",
       " 'asilah',\n",
       " 'b.a.',\n",
       " 'b.w.',\n",
       " \"bakhshi's\",\n",
       " \"bello's\",\n",
       " 'bi',\n",
       " 'biografie.',\n",
       " 'bmj.',\n",
       " \"brassey's\",\n",
       " 'c.e.',\n",
       " 'cc.',\n",
       " \"choe's\",\n",
       " 'chp.',\n",
       " 'chuch',\n",
       " 'co-',\n",
       " 'co-author',\n",
       " 'co-authored',\n",
       " 'co-edited',\n",
       " 'co-editor',\n",
       " 'co-editors',\n",
       " 'co-heiress',\n",
       " 'co-presenter',\n",
       " 'co-wrote',\n",
       " 'coca-cola',\n",
       " 'covid',\n",
       " 'd.c.',\n",
       " 'd.l.',\n",
       " 'dc.',\n",
       " 'devin-',\n",
       " 'doi:10.1136/bmj.319.7216.1031',\n",
       " 'dr.',\n",
       " 'e-',\n",
       " 'e-book',\n",
       " 'eck-verlag',\n",
       " 'ed.',\n",
       " 'edit.',\n",
       " \"einstein's\",\n",
       " 'el-',\n",
       " 'el-rufai',\n",
       " 'esq.',\n",
       " 'ethno-',\n",
       " 'f.r.c.p.',\n",
       " 'faingata',\n",
       " 'fc.',\n",
       " 'fl.',\n",
       " \"gysin's\",\n",
       " 'h.c.',\n",
       " 'ha-mishnayot',\n",
       " 'ha-ner',\n",
       " 'hada',\n",
       " \"hamri's\",\n",
       " 'hindi-',\n",
       " \"ho's\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'im',\n",
       " 'inst.',\n",
       " 'invasion`',\n",
       " 'iran-iraq',\n",
       " 'isl.',\n",
       " 'j.d.',\n",
       " 'jr.',\n",
       " 'ju-chan',\n",
       " 'kei-kwan',\n",
       " 'kingdom–based',\n",
       " \"kyoto's\",\n",
       " \"käkikoski's\",\n",
       " 'l.c.',\n",
       " 'l.r.c.p.',\n",
       " 'l.r.c.s.',\n",
       " 'lansdowne.',\n",
       " 'liechtenstein.',\n",
       " 'm.a.',\n",
       " 'm.d.',\n",
       " 'm.r.',\n",
       " 'm.r.c.p.',\n",
       " 'm.r.c.s.',\n",
       " 'mid-century',\n",
       " 'mid-day',\n",
       " 'mid-october',\n",
       " \"mitrei's\",\n",
       " 'multi-awardee',\n",
       " 'multi-instrumentalist',\n",
       " 'n.s.w.',\n",
       " 'neo-confucian',\n",
       " 'nesanice',\n",
       " 'non-anti-semitic',\n",
       " 'non-canonical',\n",
       " 'non-denominational',\n",
       " 'non-fiction',\n",
       " 'non-group',\n",
       " 'non-humorous',\n",
       " 'non-russian',\n",
       " 'novo-dmitrievakain',\n",
       " 'o-level',\n",
       " 'obi-young',\n",
       " 'otto-',\n",
       " 'p',\n",
       " 'p.j.',\n",
       " \"pbs's\",\n",
       " 'pen-name',\n",
       " 'pg.',\n",
       " 'ph.d.',\n",
       " 'pjesme',\n",
       " 'poem–',\n",
       " 'pre-eminent',\n",
       " 'pre-islamic',\n",
       " 'priče',\n",
       " 'pro-government',\n",
       " 'prof.',\n",
       " 're-election',\n",
       " 're-enactment',\n",
       " 're-reading',\n",
       " \"ruskin's\",\n",
       " 's.s.',\n",
       " 'semi-autobiographical',\n",
       " 'serbo-croatian',\n",
       " \"shin's\",\n",
       " \"son's\",\n",
       " 'son-',\n",
       " 'st.',\n",
       " 'sun-baked',\n",
       " \"suzuki's\",\n",
       " 't.c.',\n",
       " \"tambo's\",\n",
       " 'thejournal',\n",
       " \"tom's\",\n",
       " \"u'\",\n",
       " \"u'uqinak\",\n",
       " 'u-turn',\n",
       " 'u.k.',\n",
       " 'u.s.',\n",
       " \"unesco's\",\n",
       " 'uqiank',\n",
       " 'urdu-',\n",
       " 'us`',\n",
       " 'uuh',\n",
       " 'v.g.',\n",
       " 'vol.',\n",
       " 'wisconsin-madison',\n",
       " 'wisconsin–madison',\n",
       " 'yal',\n",
       " 'yonsei-',\n",
       " \"yusuf's\",\n",
       " 'zapisi',\n",
       " '¡mujeres',\n",
       " '¿sueñan',\n",
       " 'ıkavak',\n",
       " 'ς',\n",
       " 'χρήστο',\n",
       " 'школ.',\n",
       " 'ب',\n",
       " 'دقت',\n",
       " 'رفان',\n",
       " 'ع',\n",
       " '–1034',\n",
       " '–1564',\n",
       " '–17',\n",
       " '–1816',\n",
       " '–1822',\n",
       " '–1830',\n",
       " '–1863',\n",
       " '–1869',\n",
       " '–1871',\n",
       " '–1900',\n",
       " '–1905',\n",
       " '–1928',\n",
       " '–1951',\n",
       " '–1956',\n",
       " '–1974',\n",
       " '–1998',\n",
       " '–2',\n",
       " '–2002',\n",
       " '–2008',\n",
       " '–2009',\n",
       " '–2011',\n",
       " '–37',\n",
       " '–39',\n",
       " '–40',\n",
       " '–6',\n",
       " '–70',\n",
       " '–707',\n",
       " '–96',\n",
       " '–from',\n",
       " '–kishinev',\n",
       " '–march',\n",
       " '––',\n",
       " '월',\n",
       " '호'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanza_only_vocab = stanza_vocabulary.difference(spacy_vocabulary)\n",
    "stanza_only_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Stanza tends tokenize words with prefixes like `co-`, `anti-`, `non-`, `neo-`... as one token, which explains the difference with Spacy on tokenization of words of relatively formal register.\n",
    "\n",
    "We can se can see that there is the same tendency towards composed words: `pen-name`, `kingdom-based`, `serbo-croatian`...\n",
    "\n",
    "As for Named Entities, there appears to be a lot of them tokenized together with `'s`: `yusuf's`, `unesco's`, `suzuki's`, `shin's`, `kyoto's`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate common accurances we will find all tokens produced by both models for a given string and will find tokens that have the same text for both Stanza and Spacy.\n",
    "\n",
    "For this we will work with raw token entities from Stanza and Spacy to preserve PoS information for the following experiment since PoS information might be changed if a token is taken out of its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurences without sentence separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tokenize all stiched sentences together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sent_spacy_tokens = spacy_tokenize_text(shared_sents_stiched, to_string=False, no_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run for the first time Stanza tokenization:\n",
    "\"\"\" \n",
    "no_sent_stanza_tokens = stanza_tokenize_text(shared_sents_stiched, to_string=False)\n",
    "with open('SpacyStanza/no_sent_stanza_tokens.pickle', 'wb') as f:\n",
    "    pickle.dump(no_sent_stanza_tokens, f)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# To load Stanza tokens from a pickle file\n",
    "\"\"\"\"\"\"\n",
    "with open('SpacyStanza/no_sent_stanza_tokens.pickle', 'rb') as f:\n",
    "    no_sent_stanza_tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_t = common_tokens(no_sent_spacy_tokens, no_sent_stanza_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some token stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spacy tokens: 48820\n",
      "Stanza tokens: 48502\n",
      "Shared tokens: 48070\n",
      "% of shared tokens in Spacy: 98.46%\n",
      "% of shared tokens in Stanza: 99.11%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Spacy tokens: {len(no_sent_spacy_tokens)}\n",
    "Stanza tokens: {len(no_sent_stanza_tokens)}\n",
    "Shared tokens: {len(np.concatenate([x[1] for x in common_t]))}\n",
    "% of shared tokens in Spacy: {100*len(np.concatenate([x[1] for x in common_t]))/len(no_sent_spacy_tokens):.2f}%\n",
    "% of shared tokens in Stanza: {100*len(np.concatenate([x[1] for x in common_t]))/len(no_sent_stanza_tokens):.2f}%\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the results in a dictionary where the key is the word form and the value is another dictionary, storing all seen results for Spacy and Stanza separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sent_shared_tokens = {x[0][0].text: {'Spacy': x[0], 'Stanza': x[1]} for x in common_t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spacy': [record, record, record, record, record, record, record],\n",
       " 'Stanza': [[\n",
       "    {\n",
       "      \"id\": 39,\n",
       "      \"text\": \"record\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"start_char\": 13283,\n",
       "      \"end_char\": 13289\n",
       "    }\n",
       "  ],\n",
       "  [\n",
       "    {\n",
       "      \"id\": 17,\n",
       "      \"text\": \"record\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"start_char\": 27303,\n",
       "      \"end_char\": 27309\n",
       "    }\n",
       "  ],\n",
       "  [\n",
       "    {\n",
       "      \"id\": 23,\n",
       "      \"text\": \"record\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"start_char\": 95765,\n",
       "      \"end_char\": 95771\n",
       "    }\n",
       "  ],\n",
       "  [\n",
       "    {\n",
       "      \"id\": 22,\n",
       "      \"text\": \"record\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"start_char\": 117044,\n",
       "      \"end_char\": 117050\n",
       "    }\n",
       "  ],\n",
       "  [\n",
       "    {\n",
       "      \"id\": 12,\n",
       "      \"text\": \"record\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Inf\",\n",
       "      \"start_char\": 159799,\n",
       "      \"end_char\": 159805\n",
       "    }\n",
       "  ],\n",
       "  [\n",
       "    {\n",
       "      \"id\": 13,\n",
       "      \"text\": \"record\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Inf\",\n",
       "      \"start_char\": 163094,\n",
       "      \"end_char\": 163100\n",
       "    }\n",
       "  ],\n",
       "  [\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"record\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"start_char\": 214572,\n",
       "      \"end_char\": 214578\n",
       "    }\n",
       "  ]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_sent_shared_tokens['record']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>Rerun the follwing cells</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurences with segmented sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the experiment but for the separated sentences above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if you didn't run the cells above:\n",
    "\n",
    "# shared_sents_df = pd.read_csv('SpacyStanza/spacy_stanza_shared_sents.csv')\n",
    "# shared_sents_lower = [s.lower() for s in shared_sents_df.Sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_sent_tokens = list(map(lambda x: spacy_tokenize_text(x, no_filtering=True, to_string=False), shared_sents_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_sent_tokens = list(map(lambda x: stanza_tokenize_text(x, to_string=False), shared_sents_lower))\n",
    "with open('SpacyStanza/st_sent_tockens.pickle', 'wb') as f:\n",
    "    pickle.dump(st_sent_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run for the first time Stanza tokenization:\n",
    "\"\"\" \n",
    "st_sent_tokens = list(map(lambda x: stanza_tokenize_text(x, to_string=False), shared_sents_lower))\n",
    "with open('SpacyStanza/st_sent_tockens.pickle', 'wb') as f:\n",
    "    pickle.dump(st_sent_tokens, f)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# To load Stanza tokens from a pickle file\n",
    "\n",
    "with open('SpacyStanza/st_sent_tockens.pickle', 'rb') as f:\n",
    "    st_sent_tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_sent_tokens = list(map(lambda x: common_tokens(x[0], x[1]), zip(sp_sent_tokens, st_sent_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens_df = pd.DataFrame(columns=['Spacy_token_count', \n",
    "                                       'Stanza_token_count', \n",
    "                                       'Shared_token_count', \n",
    "                                       'Spacy_shared_percentage',\n",
    "                                      'Stanza_shared_percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spacy_token_count</th>\n",
       "      <th>Stanza_token_count</th>\n",
       "      <th>Shared_token_count</th>\n",
       "      <th>Spacy_shared_percentage</th>\n",
       "      <th>Stanza_shared_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>23.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>21.428571</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>9.375000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>21.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>17.391304</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spacy_token_count  Stanza_token_count  Shared_token_count  \\\n",
       "0                    50                  13                   3   \n",
       "1                    14                   5                   3   \n",
       "2                     8                  18                   2   \n",
       "3                    25                  12                   2   \n",
       "4                    10                  10                   1   \n",
       "...                 ...                 ...                 ...   \n",
       "2258                 32                  10                   3   \n",
       "2259                 16                  19                   4   \n",
       "2260                 23                  16                   4   \n",
       "2261                 15                  16                   4   \n",
       "2262                  5                  32                   1   \n",
       "\n",
       "      Spacy_shared_percentage  Stanza_shared_percentage  \n",
       "0                    6.000000                 23.076923  \n",
       "1                   21.428571                 60.000000  \n",
       "2                   25.000000                 11.111111  \n",
       "3                    8.000000                 16.666667  \n",
       "4                   10.000000                 10.000000  \n",
       "...                       ...                       ...  \n",
       "2258                 9.375000                 30.000000  \n",
       "2259                25.000000                 21.052632  \n",
       "2260                17.391304                 25.000000  \n",
       "2261                26.666667                 25.000000  \n",
       "2262                20.000000                  3.125000  \n",
       "\n",
       "[2263 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens_df.Spacy_token_count = list(map(len, sp_sent_tokens))\n",
    "sent_tokens_df.Stanza_token_count = list(map(len, st_sent_tokens))\n",
    "sent_tokens_df.Shared_token_count = list(map(len, common_sent_tokens))\n",
    "sent_tokens_df.Spacy_shared_percentage = 100 * sent_tokens_df.Shared_token_count / sent_tokens_df.Spacy_token_count \n",
    "sent_tokens_df.Stanza_shared_percentage = 100 * sent_tokens_df.Shared_token_count / sent_tokens_df.Stanza_token_count \n",
    "\n",
    "sent_tokens_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are average stats for tokens (how many tokens in a sentence are on average provided by Spacy, how many by Stanza, how many are shared etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spacy_token_count           21.573133\n",
       "Stanza_token_count          21.438356\n",
       "Shared_token_count           3.977022\n",
       "Spacy_shared_percentage     20.964089\n",
       "Stanza_shared_percentage    21.208298\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of shared tokens across all sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens_df.Shared_token_count.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have less aligned tokens now, meaning that in the experiment above we might have matched tokens from different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine all the shared tokens from our sentences into one dictionary of the same structure as above: the keys are wordforms, the values are dictionaries containing arrays of all of the occurences of the wordforms.\n",
    "\n",
    "Since for PoS analysis for Stanza requires words instead of tokens, we will convert the conversion as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_sent_tokens = {}\n",
    "\n",
    "for sent in common_sent_tokens:\n",
    "    for word in sent:\n",
    "        word_text = word[0][0].text\n",
    "        word_dict = shared_sent_tokens.get(word_text, {})\n",
    "        \n",
    "        sp_t_list = word_dict.get('Spacy', [])\n",
    "        sp_t_list.extend(word[0])\n",
    "        word_dict['Spacy'] = sp_t_list\n",
    "        \n",
    "        st_t_list = word_dict.get('Stanza', [])\n",
    "        # Before extending the list convert tokens to words\n",
    "        st_t_list.extend(list(map(lambda x: x.words[0], word[1])))\n",
    "        word_dict['Stanza'] = st_t_list\n",
    "        \n",
    "        shared_sent_tokens[word_text] = word_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how the dictionary will work on an example of one word `chief`.\n",
    "\n",
    "`shared_sent_tokens['chief']['Spacy']` can be called to get all Spacy tokens and similarly `shared_sent_tokens['chief']['Stanza']` can be called to get all Stanza tokens.\n",
    "\n",
    "In this example we can see that this word was sometimes categorized as Noun and sometimes as Adjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spacy': [chief, chief],\n",
       " 'Stanza': [{\n",
       "    \"id\": 23,\n",
       "    \"text\": \"chief\",\n",
       "    \"upos\": \"ADJ\",\n",
       "    \"xpos\": \"JJ\",\n",
       "    \"feats\": \"Degree=Pos\",\n",
       "    \"start_char\": 105,\n",
       "    \"end_char\": 110\n",
       "  },\n",
       "  {\n",
       "    \"id\": 4,\n",
       "    \"text\": \"chief\",\n",
       "    \"upos\": \"ADJ\",\n",
       "    \"xpos\": \"JJ\",\n",
       "    \"feats\": \"Degree=Pos\",\n",
       "    \"start_char\": 11,\n",
       "    \"end_char\": 16\n",
       "  }]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_sent_tokens['chief']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How PoS can be accessed for Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chief, 'NOUN')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_sent_tokens['chief']['Spacy'][0], shared_sent_tokens['chief']['Spacy'][0].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How PoS can be accessed for Stanza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({\n",
       "   \"id\": 23,\n",
       "   \"text\": \"chief\",\n",
       "   \"upos\": \"ADJ\",\n",
       "   \"xpos\": \"JJ\",\n",
       "   \"feats\": \"Degree=Pos\",\n",
       "   \"start_char\": 105,\n",
       "   \"end_char\": 110\n",
       " },\n",
       " 'ADJ')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_sent_tokens['chief']['Stanza'][0], shared_sent_tokens['chief']['Stanza'][0].upos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_stitched = ' '.join(texts)\n",
    "spacy_pos = [(x.text, x.pos_) for x in set(spacy_tokenize_text(texts_stitched, to_string=False))]\n",
    "d_spacy_pos = dict(spacy_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SpacyStanza/st_sent_tockens.pickle', 'rb') as f:\n",
    "    stanza_vocabulary = pickle.load(f)\n",
    "\n",
    "stanza_tokens = []\n",
    "for x in list(stanza_vocabulary):\n",
    "    for y in x:\n",
    "        stanza_tokens.append(y)\n",
    "\n",
    "stanza_pos = [(x.words[0].text, x.words[0].upos) for x in stanza_tokens]\n",
    "d_stanza_pos = dict(stanza_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48515, 30461)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stanza_pos), len(spacy_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pos = list(set(spacy_pos).difference(set(stanza_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9200, 0.3020255408555202, 0.18963207255487993)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_pos), len(common_pos)/len(spacy_pos), len(common_pos)/len(stanza_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PROPN': {'NOT FIND': 5272, 'X': 16, 'ADJ': 6, 'NOUN': 9}, 'NOUN': {'NOT FIND': 1644, 'VERB': 41, 'ADJ': 24, 'X': 10, 'PROPN': 2, 'NUM': 1}, 'ADJ': {'NOT FIND': 454, 'NOUN': 21, 'VERB': 9, 'ADV': 3, 'X': 3, 'PROPN': 1, 'INTJ': 1}, 'VERB': {'NOT FIND': 648, 'NOUN': 54, 'ADJ': 8, 'X': 6}, 'NUM': {'NOT FIND': 308, 'NOUN': 1}, 'X': {'NOT FIND': 35, 'PROPN': 1}, 'ADV': {'ADJ': 6, 'NOT FIND': 97, 'SCONJ': 1}, 'SCONJ': {'NOT FIND': 2}, 'PUNCT': {'NOT FIND': 5}, 'SPACE': {'NOT FIND': 4}, 'ADP': {'ADV': 1, 'SCONJ': 1, 'NOT FIND': 23, 'X': 2, 'NOUN': 1}, 'AUX': {'NOT FIND': 4, 'X': 1}, 'PRON': {'NOT FIND': 2}, 'INTJ': {'NOT FIND': 11, 'X': 1}, 'CCONJ': {'NOT FIND': 1}, 'SYM': {'NOT FIND': 1}, 'DET': {'NOT FIND': 1}}\n"
     ]
    }
   ],
   "source": [
    "d_spacy_to_stanza_pos = dict()\n",
    "for k,v in d_spacy_pos.items():\n",
    "   tmp = d_stanza_pos.get(k,'NOT FIND')\n",
    "   if (v!=tmp):\n",
    "      d_spacy_to_stanza_pos.update({v: d_spacy_to_stanza_pos.get(v,dict())})\n",
    "      d_spacy_to_stanza_pos.get(v,dict()).update({tmp: (d_spacy_to_stanza_pos.get(v,dict()).get(tmp, 0) + 1)})\n",
    "print(d_spacy_to_stanza_pos)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PROPN': {'NOT FIND': 1531, 'ADJ': 1, 'X': 1, 'NOUN': 2}, 'X': {'NOT FIND': 845, 'NOUN': 10, 'VERB': 6, 'PROPN': 16, 'ADP': 2, 'AUX': 1, 'ADJ': 3, 'INTJ': 1}, 'PUNCT': {'NOT FIND': 27}, 'SYM': {'NOT FIND': 5}, 'AUX': {'NOT FIND': 23}, 'DET': {'NOT FIND': 14}, 'ADJ': {'NOT FIND': 566, 'PROPN': 6, 'ADV': 6, 'VERB': 8, 'NOUN': 24}, 'NOUN': {'NOT FIND': 2107, 'PROPN': 9, 'VERB': 54, 'ADJ': 21, 'NUM': 1, 'ADP': 1}, 'CCONJ': {'NOT FIND': 5}, 'ADP': {'NOT FIND': 41}, 'PART': {'NOT FIND': 7}, 'ADV': {'ADP': 1, 'NOT FIND': 124, 'ADJ': 3}, 'NUM': {'NOT FIND': 252, 'NOUN': 1}, 'VERB': {'NOT FIND': 577, 'NOUN': 41, 'ADJ': 9}, 'PRON': {'NOT FIND': 36}, 'SCONJ': {'NOT FIND': 11, 'ADV': 1, 'ADP': 1}, 'INTJ': {'ADJ': 1, 'NOT FIND': 8}}\n"
     ]
    }
   ],
   "source": [
    "d_stanza_to_spacy_pos = dict()\n",
    "for k,v in d_stanza_pos.items():\n",
    "   tmp = d_spacy_pos.get(k,'NOT FIND')\n",
    "   if (v!=tmp):\n",
    "      d_stanza_to_spacy_pos.update({v: d_stanza_to_spacy_pos.get(v,dict())})\n",
    "      d_stanza_to_spacy_pos.get(v,dict()).update({tmp: (d_stanza_to_spacy_pos.get(v,dict()).get(tmp, 0) + 1)})\n",
    "print(d_stanza_to_spacy_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROPN</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>ADV</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SPACE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>AUX</th>\n",
       "      <th>PRON</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>SYM</th>\n",
       "      <th>DET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT FIND</th>\n",
       "      <td>5272.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PROPN    NOUN    ADJ   VERB    NUM     X   ADV  SCONJ  PUNCT   \n",
       "NOT FIND  5272.0  1644.0  454.0  648.0  308.0  35.0  97.0    2.0    5.0  \\\n",
       "X           16.0    10.0    3.0    6.0    0.0   0.0   0.0    0.0    0.0   \n",
       "ADJ          6.0    24.0    0.0    8.0    0.0   0.0   6.0    0.0    0.0   \n",
       "NOUN         9.0     0.0   21.0   54.0    1.0   0.0   0.0    0.0    0.0   \n",
       "VERB         0.0    41.0    9.0    0.0    0.0   0.0   0.0    0.0    0.0   \n",
       "PROPN        0.0     2.0    1.0    0.0    0.0   1.0   0.0    0.0    0.0   \n",
       "NUM          0.0     1.0    0.0    0.0    0.0   0.0   0.0    0.0    0.0   \n",
       "ADV          0.0     0.0    3.0    0.0    0.0   0.0   0.0    0.0    0.0   \n",
       "INTJ         0.0     0.0    1.0    0.0    0.0   0.0   0.0    0.0    0.0   \n",
       "SCONJ        0.0     0.0    0.0    0.0    0.0   0.0   1.0    0.0    0.0   \n",
       "\n",
       "          SPACE   ADP  AUX  PRON  INTJ  CCONJ  SYM  DET  \n",
       "NOT FIND    4.0  23.0  4.0   2.0  11.0    1.0  1.0  1.0  \n",
       "X           0.0   2.0  1.0   0.0   1.0    0.0  0.0  0.0  \n",
       "ADJ         0.0   0.0  0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "NOUN        0.0   1.0  0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "VERB        0.0   0.0  0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "PROPN       0.0   0.0  0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "NUM         0.0   0.0  0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "ADV         0.0   1.0  0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "INTJ        0.0   0.0  0.0   0.0   0.0    0.0  0.0  0.0  \n",
       "SCONJ       0.0   1.0  0.0   0.0   0.0    0.0  0.0  0.0  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spacy_to_stanza = pd.DataFrame(d_spacy_to_stanza_pos).fillna(0)\n",
    "df_spacy_to_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROPN</th>\n",
       "      <th>X</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SYM</th>\n",
       "      <th>AUX</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>PART</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NUM</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRON</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>INTJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT FIND</th>\n",
       "      <td>1531.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>2107.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PROPN      X  PUNCT  SYM   AUX   DET    ADJ    NOUN  CCONJ   ADP   \n",
       "NOT FIND  1531.0  845.0   27.0  5.0  23.0  14.0  566.0  2107.0    5.0  41.0  \\\n",
       "ADJ          1.0    3.0    0.0  0.0   0.0   0.0    0.0    21.0    0.0   0.0   \n",
       "X            1.0    0.0    0.0  0.0   0.0   0.0    0.0     0.0    0.0   0.0   \n",
       "NOUN         2.0   10.0    0.0  0.0   0.0   0.0   24.0     0.0    0.0   0.0   \n",
       "VERB         0.0    6.0    0.0  0.0   0.0   0.0    8.0    54.0    0.0   0.0   \n",
       "PROPN        0.0   16.0    0.0  0.0   0.0   0.0    6.0     9.0    0.0   0.0   \n",
       "ADP          0.0    2.0    0.0  0.0   0.0   0.0    0.0     1.0    0.0   0.0   \n",
       "AUX          0.0    1.0    0.0  0.0   0.0   0.0    0.0     0.0    0.0   0.0   \n",
       "INTJ         0.0    1.0    0.0  0.0   0.0   0.0    0.0     0.0    0.0   0.0   \n",
       "ADV          0.0    0.0    0.0  0.0   0.0   0.0    6.0     0.0    0.0   0.0   \n",
       "NUM          0.0    0.0    0.0  0.0   0.0   0.0    0.0     1.0    0.0   0.0   \n",
       "\n",
       "          PART    ADV    NUM   VERB  PRON  SCONJ  INTJ  \n",
       "NOT FIND   7.0  124.0  252.0  577.0  36.0   11.0   8.0  \n",
       "ADJ        0.0    3.0    0.0    9.0   0.0    0.0   1.0  \n",
       "X          0.0    0.0    0.0    0.0   0.0    0.0   0.0  \n",
       "NOUN       0.0    0.0    1.0   41.0   0.0    0.0   0.0  \n",
       "VERB       0.0    0.0    0.0    0.0   0.0    0.0   0.0  \n",
       "PROPN      0.0    0.0    0.0    0.0   0.0    0.0   0.0  \n",
       "ADP        0.0    1.0    0.0    0.0   0.0    1.0   0.0  \n",
       "AUX        0.0    0.0    0.0    0.0   0.0    0.0   0.0  \n",
       "INTJ       0.0    0.0    0.0    0.0   0.0    0.0   0.0  \n",
       "ADV        0.0    0.0    0.0    0.0   0.0    1.0   0.0  \n",
       "NUM        0.0    0.0    0.0    0.0   0.0    0.0   0.0  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stanza_to_spacy = pd.DataFrame(d_stanza_to_spacy_pos).fillna(0)\n",
    "df_stanza_to_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROPN</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>ADV</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SPACE</th>\n",
       "      <th>...</th>\n",
       "      <th>SCONJ%%%</th>\n",
       "      <th>PUNCT%%%</th>\n",
       "      <th>SPACE%%%</th>\n",
       "      <th>ADP%%%</th>\n",
       "      <th>AUX%%%</th>\n",
       "      <th>PRON%%%</th>\n",
       "      <th>INTJ%%%</th>\n",
       "      <th>CCONJ%%%</th>\n",
       "      <th>SYM%%%</th>\n",
       "      <th>DET%%%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT FIND</th>\n",
       "      <td>5272.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PROPN    NOUN    ADJ   VERB    NUM     X   ADV  SCONJ  PUNCT   \n",
       "NOT FIND  5272.0  1644.0  454.0  648.0  308.0  35.0  97.0    2.0    5.0  \\\n",
       "X           16.0    10.0    3.0    6.0    0.0   0.0   0.0    0.0    0.0   \n",
       "ADJ          6.0    24.0    0.0    8.0    0.0   0.0   6.0    0.0    0.0   \n",
       "NOUN         9.0     0.0   21.0   54.0    1.0   0.0   0.0    0.0    0.0   \n",
       "VERB         0.0    41.0    9.0    0.0    0.0   0.0   0.0    0.0    0.0   \n",
       "PROPN        0.0     2.0    1.0    0.0    0.0   1.0   0.0    0.0    0.0   \n",
       "NUM          0.0     1.0    0.0    0.0    0.0   0.0   0.0    0.0    0.0   \n",
       "ADV          0.0     0.0    3.0    0.0    0.0   0.0   0.0    0.0    0.0   \n",
       "INTJ         0.0     0.0    1.0    0.0    0.0   0.0   0.0    0.0    0.0   \n",
       "SCONJ        0.0     0.0    0.0    0.0    0.0   0.0   1.0    0.0    0.0   \n",
       "\n",
       "          SPACE  ...  SCONJ%%%  PUNCT%%%  SPACE%%%    ADP%%%  AUX%%%  PRON%%%   \n",
       "NOT FIND    4.0  ...       1.0       1.0       1.0  0.821429     0.8      1.0  \\\n",
       "X           0.0  ...       0.0       0.0       0.0  0.071429     0.2      0.0   \n",
       "ADJ         0.0  ...       0.0       0.0       0.0  0.000000     0.0      0.0   \n",
       "NOUN        0.0  ...       0.0       0.0       0.0  0.035714     0.0      0.0   \n",
       "VERB        0.0  ...       0.0       0.0       0.0  0.000000     0.0      0.0   \n",
       "PROPN       0.0  ...       0.0       0.0       0.0  0.000000     0.0      0.0   \n",
       "NUM         0.0  ...       0.0       0.0       0.0  0.000000     0.0      0.0   \n",
       "ADV         0.0  ...       0.0       0.0       0.0  0.035714     0.0      0.0   \n",
       "INTJ        0.0  ...       0.0       0.0       0.0  0.000000     0.0      0.0   \n",
       "SCONJ       0.0  ...       0.0       0.0       0.0  0.035714     0.0      0.0   \n",
       "\n",
       "           INTJ%%%  CCONJ%%%  SYM%%%  DET%%%  \n",
       "NOT FIND  0.916667       1.0     1.0     1.0  \n",
       "X         0.083333       0.0     0.0     0.0  \n",
       "ADJ       0.000000       0.0     0.0     0.0  \n",
       "NOUN      0.000000       0.0     0.0     0.0  \n",
       "VERB      0.000000       0.0     0.0     0.0  \n",
       "PROPN     0.000000       0.0     0.0     0.0  \n",
       "NUM       0.000000       0.0     0.0     0.0  \n",
       "ADV       0.000000       0.0     0.0     0.0  \n",
       "INTJ      0.000000       0.0     0.0     0.0  \n",
       "SCONJ     0.000000       0.0     0.0     0.0  \n",
       "\n",
       "[10 rows x 68 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df_spacy_to_stanza.columns:\n",
    "    df_spacy_to_stanza[i+'%'] = df_spacy_to_stanza[i]/df_spacy_to_stanza[i].sum()\n",
    "df_spacy_to_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROPN</th>\n",
       "      <th>X</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SYM</th>\n",
       "      <th>AUX</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>...</th>\n",
       "      <th>NOUN%</th>\n",
       "      <th>CCONJ%</th>\n",
       "      <th>ADP%</th>\n",
       "      <th>PART%</th>\n",
       "      <th>ADV%</th>\n",
       "      <th>NUM%</th>\n",
       "      <th>VERB%</th>\n",
       "      <th>PRON%</th>\n",
       "      <th>SCONJ%</th>\n",
       "      <th>INTJ%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT FIND</th>\n",
       "      <td>1531.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>2107.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.996047</td>\n",
       "      <td>0.920255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.065391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PROPN      X  PUNCT  SYM   AUX   DET    ADJ    NOUN  CCONJ   ADP   \n",
       "NOT FIND  1531.0  845.0   27.0  5.0  23.0  14.0  566.0  2107.0    5.0  41.0  \\\n",
       "ADJ          1.0    3.0    0.0  0.0   0.0   0.0    0.0    21.0    0.0   0.0   \n",
       "X            1.0    0.0    0.0  0.0   0.0   0.0    0.0     0.0    0.0   0.0   \n",
       "NOUN         2.0   10.0    0.0  0.0   0.0   0.0   24.0     0.0    0.0   0.0   \n",
       "VERB         0.0    6.0    0.0  0.0   0.0   0.0    8.0    54.0    0.0   0.0   \n",
       "PROPN        0.0   16.0    0.0  0.0   0.0   0.0    6.0     9.0    0.0   0.0   \n",
       "ADP          0.0    2.0    0.0  0.0   0.0   0.0    0.0     1.0    0.0   0.0   \n",
       "AUX          0.0    1.0    0.0  0.0   0.0   0.0    0.0     0.0    0.0   0.0   \n",
       "INTJ         0.0    1.0    0.0  0.0   0.0   0.0    0.0     0.0    0.0   0.0   \n",
       "ADV          0.0    0.0    0.0  0.0   0.0   0.0    6.0     0.0    0.0   0.0   \n",
       "NUM          0.0    0.0    0.0  0.0   0.0   0.0    0.0     1.0    0.0   0.0   \n",
       "\n",
       "          ...     NOUN%  CCONJ%  ADP%  PART%      ADV%      NUM%     VERB%   \n",
       "NOT FIND  ...  0.960784     1.0   1.0    1.0  0.968750  0.996047  0.920255  \\\n",
       "ADJ       ...  0.009576     0.0   0.0    0.0  0.023438  0.000000  0.014354   \n",
       "X         ...  0.000000     0.0   0.0    0.0  0.000000  0.000000  0.000000   \n",
       "NOUN      ...  0.000000     0.0   0.0    0.0  0.000000  0.003953  0.065391   \n",
       "VERB      ...  0.024624     0.0   0.0    0.0  0.000000  0.000000  0.000000   \n",
       "PROPN     ...  0.004104     0.0   0.0    0.0  0.000000  0.000000  0.000000   \n",
       "ADP       ...  0.000456     0.0   0.0    0.0  0.007812  0.000000  0.000000   \n",
       "AUX       ...  0.000000     0.0   0.0    0.0  0.000000  0.000000  0.000000   \n",
       "INTJ      ...  0.000000     0.0   0.0    0.0  0.000000  0.000000  0.000000   \n",
       "ADV       ...  0.000000     0.0   0.0    0.0  0.000000  0.000000  0.000000   \n",
       "NUM       ...  0.000456     0.0   0.0    0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "          PRON%    SCONJ%     INTJ%  \n",
       "NOT FIND    1.0  0.846154  0.888889  \n",
       "ADJ         0.0  0.000000  0.111111  \n",
       "X           0.0  0.000000  0.000000  \n",
       "NOUN        0.0  0.000000  0.000000  \n",
       "VERB        0.0  0.000000  0.000000  \n",
       "PROPN       0.0  0.000000  0.000000  \n",
       "ADP         0.0  0.076923  0.000000  \n",
       "AUX         0.0  0.000000  0.000000  \n",
       "INTJ        0.0  0.000000  0.000000  \n",
       "ADV         0.0  0.076923  0.000000  \n",
       "NUM         0.0  0.000000  0.000000  \n",
       "\n",
       "[11 rows x 34 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df_stanza_to_spacy.columns:\n",
    "    df_stanza_to_spacy[i+'%'] = df_stanza_to_spacy[i]/df_stanza_to_spacy[i].sum()\n",
    "df_stanza_to_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5cfb0889301bd40a32af2921b094f94edad3784ee7c8bac1076d6edd892f10d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
